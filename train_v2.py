"""
训练脚本 V2 - 纯图像对的Sim2Real Flow Matching
无需prompt，端到端训练
"""
import os
import sys
import argparse
from pathlib import Path
import yaml
import random
import numpy as np
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from models_v2 import Sim2RealFlowModel
from models_v2.perceptual_loss import PerceptualLoss
from utils_v2 import RDPairDataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms


class EarlyStopping:
    """早停机制"""
    def __init__(self, patience=20, min_delta=0.0001, monitor='val_loss'):
        self.patience = patience
        self.min_delta = min_delta
        self.monitor = monitor
        self.counter = 0
        self.best_score = None
        self.early_stop = False
    
    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
        elif score > self.best_score - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0
        
        return self.early_stop


class Trainer:
    def __init__(self, config_path):
        # 加载配置
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.setup_environment()
        self.setup_paths()
        self.setup_model()
        self.setup_data()
        self.setup_optimizer()
        self.setup_training()
        
        print("="*60)
        print("训练配置:")
        print(f"  模型: Sim2RealFlowModel V2")
        print(f"  参数量: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"  训练集: {len(self.train_loader.dataset)}")
        print(f"  验证集: {len(self.val_loader.dataset)}")
        print(f"  批大小: {self.config['train']['batch_size']}")
        print(f"  梯度累积: {self.config['train']['gradient_accumulation_steps']}")
        print(f"  实际批大小: {self.config['train']['batch_size'] * self.config['train']['gradient_accumulation_steps']}")
        print(f"  学习率: {self.config['train']['learning_rate']}")
        print(f"  混合精度: {self.config['train']['mixed_precision']}")
        print(f"  感知损失: {self.config['loss']['use_perceptual']} (权重={self.config['loss']['perceptual_weight']})")
        print("="*60)
    
    def setup_environment(self):
        """设置环境"""
        # 随机种子
        seed = self.config.get('misc', {}).get('seed', 42)
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        
        # CuDNN
        if self.config.get('misc', {}).get('cudnn_benchmark', True):
            torch.backends.cudnn.benchmark = True
        if self.config.get('misc', {}).get('deterministic', False):
            torch.backends.cudnn.deterministic = True
        
        # 设备
        self.device = torch.device(self.config['train']['device'])
        print(f"使用设备: {self.device}")
    
    def setup_paths(self):
        """创建输出目录"""
        self.output_dir = Path(self.config['paths']['output_dir'])
        self.log_dir = Path(self.config['paths']['log_dir'])
        self.checkpoint_dir = Path(self.config['paths']['checkpoint_dir'])
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir=str(self.log_dir))
    
    def setup_model(self):
        """创建模型"""
        model_cfg = self.config['model']
        
        self.model = Sim2RealFlowModel(
            base_channels=model_cfg['base_channels'],
            channel_mult=tuple(model_cfg['channel_mult']),
            time_embed_dim=model_cfg['time_embed_dim'],
            num_res_blocks=model_cfg['num_res_blocks'],
            attention_levels=tuple(model_cfg['attention_levels']),
            dropout=model_cfg['dropout']
        ).to(self.device)
        
        # Perceptual Loss
        if self.config['loss']['use_perceptual']:
            self.perceptual_criterion = PerceptualLoss(
                feature_layers=tuple(self.config['loss']['perceptual_layers'])
            ).to(self.device)
        else:
            self.perceptual_criterion = None
        
        # 恢复训练
        self.start_epoch = 0
        self.global_step = 0
        self.best_val_loss = float('inf')
        
        if self.config['resume']['checkpoint']:
            self.load_checkpoint(self.config['resume']['checkpoint'])
    
    def setup_data(self):
        """创建数据加载器"""
        # 图像变换
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[self.config['data']['normalize_mean']],
                std=[self.config['data']['normalize_std']]
            )
        ])
        
        # 训练集
        train_dataset = RDPairDataset(
            data_root=self.config['data']['train_root'],
            transform=transform,
            augment=self.config['data']['augment']
        )
        
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['train']['batch_size'],
            shuffle=True,
            num_workers=self.config['train']['num_workers'],
            pin_memory=True,
            drop_last=True
        )
        
        # 验证集
        val_dataset = RDPairDataset(
            data_root=self.config['data']['val_root'],
            transform=transform,
            augment=False
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['train']['batch_size'],
            shuffle=False,
            num_workers=self.config['train']['num_workers'],
            pin_memory=True,
            drop_last=False
        )
    
    def setup_optimizer(self):
        """创建优化器和调度器"""
        # 优化器
        if self.config['train']['optimizer'] == 'adamw':
            self.optimizer = torch.optim.AdamW(
                self.model.parameters(),
                lr=self.config['train']['learning_rate'],
                betas=tuple(self.config['train']['betas']),
                weight_decay=self.config['train']['weight_decay']
            )
        else:
            self.optimizer = torch.optim.Adam(
                self.model.parameters(),
                lr=self.config['train']['learning_rate'],
                betas=tuple(self.config['train']['betas'])
            )
        
        # 学习率调度器
        if self.config['train']['lr_scheduler'] == 'cosine':
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['train']['num_epochs'] - self.config['train']['lr_warmup_epochs'],
                eta_min=self.config['train']['lr_min']
            )
        elif self.config['train']['lr_scheduler'] == 'step':
            self.scheduler = torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=30,
                gamma=0.1
            )
        elif self.config['train']['lr_scheduler'] == 'plateau':
            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=0.5,
                patience=10,
                verbose=True
            )
        else:
            self.scheduler = None
        
        # 混合精度
        self.scaler = torch.amp.GradScaler('cuda') if self.config['train']['mixed_precision'] else None
    
    def setup_training(self):
        """设置训练相关"""
        # 早停
        if self.config['train']['early_stopping']['enabled']:
            self.early_stopping = EarlyStopping(
                patience=self.config['train']['early_stopping']['patience'],
                min_delta=self.config['train']['early_stopping']['min_delta'],
                monitor=self.config['train']['early_stopping']['monitor']
            )
        else:
            self.early_stopping = None
        
        # 梯度累积
        self.grad_accum_steps = self.config['train']['gradient_accumulation_steps']
    
    def train_one_epoch(self, epoch):
        """训练一个epoch"""
        self.model.train()
        
        total_loss = 0
        total_loss_fm = 0
        total_loss_perceptual = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}/{self.config['train']['num_epochs']}")
        
        for batch_idx, (sim_images, real_images, _) in enumerate(pbar):
            sim_images = sim_images.to(self.device)
            real_images = real_images.to(self.device)
            
            # 混合精度
            with torch.amp.autocast('cuda', enabled=self.config['train']['mixed_precision']):
                # Flow Matching Loss
                loss_fm = self.model.compute_loss(sim_images, real_images)
                
                # Perceptual Loss（简化版：直接用真实图和当前ODE中间步骤）
                loss_perceptual = torch.tensor(0.0, device=self.device)
                if (self.perceptual_criterion is not None and 
                    self.global_step % self.config['loss']['perceptual_interval'] == 0):
                    # 使用t=0.5时刻的预测作为中间结果（避免完整ODE求解）
                    batch_size = sim_images.shape[0]
                    t_mid = torch.ones(batch_size, device=self.device) * 0.5
                    noise = torch.randn_like(real_images)
                    x_mid = 0.5 * noise + 0.5 * real_images
                    v_pred = self.model(x_mid, t_mid, sim_images)
                    # 简单预测结果
                    predicted = x_mid + v_pred * 0.5
                    loss_perceptual = self.perceptual_criterion(predicted, real_images)
                
                # 总Loss
                loss = loss_fm + self.config['loss']['perceptual_weight'] * loss_perceptual
                loss = loss / self.grad_accum_steps
            
            # 反向传播
            if self.scaler:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # 梯度累积
            if (batch_idx + 1) % self.grad_accum_steps == 0:
                # 梯度裁剪
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config['train']['max_grad_norm']
                )
                
                # 更新参数
                if self.scaler:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
                
                self.optimizer.zero_grad()
            
            # 统计
            total_loss += loss.item() * self.grad_accum_steps
            total_loss_fm += loss_fm.item()
            if loss_perceptual.item() > 0:
                total_loss_perceptual += loss_perceptual.item()
            
            # 日志
            if self.global_step % self.config['train']['log_interval'] == 0:
                self.writer.add_scalar('train/loss', loss.item() * self.grad_accum_steps, self.global_step)
                self.writer.add_scalar('train/loss_fm', loss_fm.item(), self.global_step)
                if loss_perceptual.item() > 0:
                    self.writer.add_scalar('train/loss_perceptual', loss_perceptual.item(), self.global_step)
                self.writer.add_scalar('train/lr', self.optimizer.param_groups[0]['lr'], self.global_step)
            
            pbar.set_postfix({
                'loss': f"{loss.item() * self.grad_accum_steps:.4f}",
                'loss_fm': f"{loss_fm.item():.4f}",
                'lr': f"{self.optimizer.param_groups[0]['lr']:.6f}"
            })
            
            self.global_step += 1
        
        avg_loss = total_loss / len(self.train_loader)
        avg_loss_fm = total_loss_fm / len(self.train_loader)
        
        return avg_loss, avg_loss_fm
    
    @torch.no_grad()
    def validate(self, epoch):
        """验证"""
        self.model.eval()
        
        total_loss = 0
        total_loss_fm = 0
        
        for sim_images, real_images, _ in tqdm(self.val_loader, desc="Validation"):
            sim_images = sim_images.to(self.device)
            real_images = real_images.to(self.device)
            
            # Flow Matching Loss
            loss_fm = self.model.compute_loss(sim_images, real_images)
            
            total_loss += loss_fm.item()
            total_loss_fm += loss_fm.item()
        
        avg_loss = total_loss / len(self.val_loader)
        avg_loss_fm = total_loss_fm / len(self.val_loader)
        
        # 记录
        self.writer.add_scalar('val/loss', avg_loss, epoch)
        self.writer.add_scalar('val/loss_fm', avg_loss_fm, epoch)
        
        return avg_loss
    
    def save_checkpoint(self, epoch, is_best=False):
        """保存检查点"""
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        
        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # 保存最新
        checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        print(f"保存检查点: {checkpoint_path}")
        
        # 保存最佳
        if is_best:
            best_path = self.checkpoint_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"保存最佳模型: {best_path}")
        
        # 清理旧检查点
        keep_n = self.config['train']['keep_last_n_checkpoints']
        if keep_n > 0:
            checkpoints = sorted(self.checkpoint_dir.glob("checkpoint_epoch_*.pth"))
            for ckpt in checkpoints[:-keep_n]:
                ckpt.unlink()
    
    def load_checkpoint(self, checkpoint_path):
        """加载检查点"""
        print(f"加载检查点: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if checkpoint['scheduler_state_dict'] and self.scheduler:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.start_epoch = checkpoint['epoch'] + 1
        self.global_step = checkpoint['global_step']
        self.best_val_loss = checkpoint['best_val_loss']
        
        if self.scaler and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
    
    def train(self):
        """主训练循环"""
        print("\n开始训练...")
        
        for epoch in range(self.start_epoch, self.config['train']['num_epochs']):
            # 训练
            train_loss, train_loss_fm = self.train_one_epoch(epoch)
            
            # 验证
            val_loss = self.validate(epoch)
            
            # 记录epoch级别的指标
            self.writer.add_scalar('epoch/train_loss', train_loss, epoch)
            self.writer.add_scalar('epoch/val_loss', val_loss, epoch)
            
            print(f"\nEpoch {epoch}:")
            print(f"  Train Loss: {train_loss:.6f} (FM: {train_loss_fm:.6f})")
            print(f"  Val Loss: {val_loss:.6f}")
            
            # 学习率调度
            if self.scheduler:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()
            
            # 保存检查点
            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss
            
            if (epoch + 1) % self.config['train']['save_interval'] == 0:
                self.save_checkpoint(epoch, is_best)
            
            # 早停
            if self.early_stopping:
                if self.early_stopping(val_loss):
                    print(f"\n早停触发！最佳Val Loss: {self.best_val_loss:.6f}")
                    break
        
        print("\n训练完成！")
        self.writer.close()


def main():
    parser = argparse.ArgumentParser(description="Train Sim2Real Flow Matching V2")
    parser.add_argument('--config', type=str, default='config_v2.yaml', help='配置文件路径')
    args = parser.parse_args()
    
    trainer = Trainer(args.config)
    trainer.train()


if __name__ == "__main__":
    main()

