"""
è®­ç»ƒè„šæœ¬ V2 - çº¯å›¾åƒå¯¹çš„Sim2Real Flow Matching
æ— éœ€promptï¼Œç«¯åˆ°ç«¯è®­ç»ƒ
"""
import os
import sys
import argparse
from pathlib import Path
import yaml
import random
import numpy as np
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from models_v2 import Sim2RealFlowModel
from models_v2.perceptual_loss import PerceptualLoss
from utils_v2 import RDPairDataset
from utils_v2.losses import frequency_domain_loss, ssim_loss
from torch.utils.data import DataLoader
import torchvision.transforms as transforms


class EarlyStopping:
    """æ—©åœæœºåˆ¶
    
    å¯¹äºlossæŒ‡æ ‡ï¼ˆè¶Šå°è¶Šå¥½ï¼‰ï¼š
    - å¦‚æœ score < best_score - min_deltaï¼šè®¤ä¸ºæœ‰æ”¹å–„ï¼Œæ›´æ–°best_scoreå¹¶é‡ç½®counter
    - å¦‚æœ score >= best_score - min_deltaï¼šè®¤ä¸ºæ²¡æœ‰æ”¹å–„æˆ–æ”¹å–„ä¸è¶³ï¼Œcounterå¢åŠ 
    - å½“counter >= patienceæ—¶ï¼Œè§¦å‘æ—©åœ
    """
    def __init__(self, patience=20, min_delta=0.0001, monitor='val_loss'):
        self.patience = patience
        self.min_delta = min_delta
        self.monitor = monitor
        self.counter = 0
        self.best_score = None
        self.early_stop = False
    
    def __call__(self, score):
        """
        Args:
            score: å½“å‰æŒ‡æ ‡å€¼ï¼ˆå¯¹äºlossï¼Œè¶Šå°è¶Šå¥½ï¼‰
        Returns:
            bool: æ˜¯å¦è§¦å‘æ—©åœ
        """
        if self.best_score is None:
            # ç¬¬ä¸€ä¸ªepochï¼Œåˆå§‹åŒ–best_score
            self.best_score = score
        elif score > self.best_score - self.min_delta:
            # æ²¡æœ‰æ”¹å–„æˆ–æ”¹å–„ä¸è¶³min_deltaï¼ˆå¯¹äºlossï¼šscoreè¶Šå¤§è¡¨ç¤ºè¶Šå·®ï¼‰
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            # æœ‰æ”¹å–„ï¼ˆscore < best_score - min_deltaï¼Œå¯¹äºlossè¡¨ç¤ºé™ä½äº†è‡³å°‘min_deltaï¼‰
            self.best_score = score
            self.counter = 0
        
        return self.early_stop


class Trainer:
    def __init__(self, config_path):
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.setup_environment()
        self.setup_paths()
        self.setup_model()
        self.setup_data()
        self.setup_optimizer()
        self.setup_training()
        
        print("="*60)
        print("è®­ç»ƒé…ç½®:")
        print(f"  æ¨¡å‹: Sim2RealFlowModel V2")
        print(f"  å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"  è®­ç»ƒé›†: {len(self.train_loader.dataset)}")
        print(f"  éªŒè¯é›†: {len(self.val_loader.dataset)}")
        print(f"  æ‰¹å¤§å°: {self.config['train']['batch_size']}")
        print(f"  æ¢¯åº¦ç´¯ç§¯: {self.config['train']['gradient_accumulation_steps']}")
        print(f"  å®é™…æ‰¹å¤§å°: {self.config['train']['batch_size'] * self.config['train']['gradient_accumulation_steps']}")
        print(f"  å­¦ä¹ ç‡: {self.config['train']['learning_rate']}")
        print(f"  æ··åˆç²¾åº¦: {self.config['train']['mixed_precision']}")
        print(f"  æ„ŸçŸ¥æŸå¤±: {self.config['loss']['use_perceptual']} (æƒé‡={self.config['loss']['perceptual_weight']})")
        print("="*60)
    
    def setup_environment(self):
        """è®¾ç½®ç¯å¢ƒ"""
        # éšæœºç§å­
        seed = self.config.get('misc', {}).get('seed', 42)
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        
        # CuDNN
        if self.config.get('misc', {}).get('cudnn_benchmark', True):
            torch.backends.cudnn.benchmark = True
        if self.config.get('misc', {}).get('deterministic', False):
            torch.backends.cudnn.deterministic = True
        
        # è®¾å¤‡
        self.device = torch.device(self.config['train']['device'])
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def setup_paths(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        self.output_dir = Path(self.config['paths']['output_dir'])
        self.log_dir = Path(self.config['paths']['log_dir'])
        self.checkpoint_dir = Path(self.config['paths']['checkpoint_dir'])
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir=str(self.log_dir))
    
    def setup_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        model_cfg = self.config['model']
        
        self.model = Sim2RealFlowModel(
            base_channels=model_cfg['base_channels'],
            channel_mult=tuple(model_cfg['channel_mult']),
            time_embed_dim=model_cfg['time_embed_dim'],
            num_res_blocks=model_cfg['num_res_blocks'],
            attention_levels=tuple(model_cfg['attention_levels']),
            dropout=model_cfg['dropout']
        ).to(self.device)
        
        # Perceptual Loss
        if self.config['loss']['use_perceptual']:
            self.perceptual_criterion = PerceptualLoss(
                feature_layers=tuple(self.config['loss']['perceptual_layers'])
            ).to(self.device)
        else:
            self.perceptual_criterion = None
        
        # æ¢å¤è®­ç»ƒ
        self.start_epoch = 1  # ğŸ”§ ç»Ÿä¸€ï¼šepochä»1å¼€å§‹
        self.global_step = 0
        self.best_val_loss = float('inf')
        
        if self.config['resume']['checkpoint']:
            self.load_checkpoint(self.config['resume']['checkpoint'])
    
    def setup_data(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        # å›¾åƒå˜æ¢
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[self.config['data']['normalize_mean']],
                std=[self.config['data']['normalize_std']]
            )
        ])
        
        # è®­ç»ƒé›†
        train_dataset = RDPairDataset(
            data_root=self.config['data']['train_root'],
            transform=transform,
            augment=self.config['data']['augment']
        )
        
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['train']['batch_size'],
            shuffle=True,
            num_workers=self.config['train']['num_workers'],
            pin_memory=True,
            drop_last=True
        )
        
        # éªŒè¯é›†
        val_dataset = RDPairDataset(
            data_root=self.config['data']['val_root'],
            transform=transform,
            augment=False
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['train']['batch_size'],
            shuffle=False,
            num_workers=self.config['train']['num_workers'],
            pin_memory=True,
            drop_last=False
        )
    
    def setup_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
        # ä¼˜åŒ–å™¨
        if self.config['train']['optimizer'] == 'adamw':
            self.optimizer = torch.optim.AdamW(
                self.model.parameters(),
                lr=self.config['train']['learning_rate'],
                betas=tuple(self.config['train']['betas']),
                weight_decay=self.config['train']['weight_decay']
            )
        else:
            self.optimizer = torch.optim.Adam(
                self.model.parameters(),
                lr=self.config['train']['learning_rate'],
                betas=tuple(self.config['train']['betas'])
            )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config['train']['lr_scheduler'] == 'cosine':
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['train']['num_epochs'] - self.config['train']['lr_warmup_epochs'],
                eta_min=self.config['train']['lr_min']
            )
        elif self.config['train']['lr_scheduler'] == 'step':
            self.scheduler = torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=30,
                gamma=0.1
            )
        elif self.config['train']['lr_scheduler'] == 'plateau':
            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode=self.config['train'].get('plateau_mode', 'min'),
                factor=self.config['train'].get('plateau_factor', 0.5),
                patience=self.config['train'].get('plateau_patience', 10),
                min_lr=self.config['train'].get('plateau_min_lr', 1e-6)
            )
        else:
            self.scheduler = None
        
        # æ··åˆç²¾åº¦
        self.scaler = torch.amp.GradScaler('cuda') if self.config['train']['mixed_precision'] else None
    
    def setup_training(self):
        """è®¾ç½®è®­ç»ƒç›¸å…³"""
        # æ—©åœ
        if self.config['train']['early_stopping']['enabled']:
            self.early_stopping = EarlyStopping(
                patience=self.config['train']['early_stopping']['patience'],
                min_delta=self.config['train']['early_stopping']['min_delta'],
                monitor=self.config['train']['early_stopping']['monitor']
            )
        else:
            self.early_stopping = None
        
        # æ¢¯åº¦ç´¯ç§¯
        self.grad_accum_steps = self.config['train']['gradient_accumulation_steps']
    
    def train_one_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0
        total_loss_fm = 0
        total_loss_perceptual = 0
        total_loss_frequency = 0
        total_loss_ssim = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}/{self.config['train']['num_epochs']}")
        
        for batch_idx, (sim_images, real_images, _) in enumerate(pbar):
            sim_images = sim_images.to(self.device)
            real_images = real_images.to(self.device)
            
            # æ··åˆç²¾åº¦
            with torch.amp.autocast('cuda', enabled=self.config['train']['mixed_precision']):
                # Flow Matching Loss
                loss_fm = self.model.compute_loss(sim_images, real_images)
                
                # è·å–ä¸€ä¸ªé¢„æµ‹æ ·æœ¬ç”¨äºç»“æ„lossè®¡ç®—
                # ä½¿ç”¨t=0.5æ—¶åˆ»çš„é¢„æµ‹ä½œä¸ºä¸­é—´ç»“æœï¼ˆé¿å…å®Œæ•´ODEæ±‚è§£ï¼‰
                batch_size = sim_images.shape[0]
                t_mid = torch.ones(batch_size, device=self.device) * 0.5
                noise = torch.randn_like(real_images)
                x_mid = 0.5 * noise + 0.5 * real_images
                v_pred = self.model(x_mid, t_mid, sim_images)
                # ç®€å•é¢„æµ‹ç»“æœ
                predicted = x_mid + v_pred * 0.5
                
                # Perceptual Lossï¼ˆç®€åŒ–ç‰ˆï¼‰
                loss_perceptual = torch.tensor(0.0, device=self.device)
                if (self.perceptual_criterion is not None and 
                    self.global_step % self.config['loss']['perceptual_interval'] == 0):
                    loss_perceptual = self.perceptual_criterion(predicted, real_images)
                
                # é¢‘åŸŸLossï¼ˆå­¦ä¹ å¤šæ™®å‹’ç»“æ„ï¼‰
                loss_freq = torch.tensor(0.0, device=self.device)
                if self.config['loss'].get('use_frequency', False):
                    loss_freq = frequency_domain_loss(predicted, real_images)
                
                # SSIM Lossï¼ˆç»“æ„ç›¸ä¼¼æ€§ï¼‰
                loss_ssim_val = torch.tensor(0.0, device=self.device)
                if self.config['loss'].get('use_ssim', False):
                    loss_ssim_val = ssim_loss(predicted, real_images)
                
                # æ€»Loss
                loss = (
                    loss_fm + 
                    self.config['loss']['perceptual_weight'] * loss_perceptual +
                    self.config['loss'].get('frequency_weight', 0.1) * loss_freq +
                    self.config['loss'].get('ssim_weight', 0.3) * loss_ssim_val
                )
                loss = loss / self.grad_accum_steps
            
            # åå‘ä¼ æ’­
            if self.scaler:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # æ¢¯åº¦ç´¯ç§¯
            if (batch_idx + 1) % self.grad_accum_steps == 0:
                # æ¢¯åº¦è£å‰ª
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config['train']['max_grad_norm']
                )
                
                # æ›´æ–°å‚æ•°
                if self.scaler:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
                
                self.optimizer.zero_grad()
            
            # ç»Ÿè®¡
            total_loss += loss.item() * self.grad_accum_steps
            total_loss_fm += loss_fm.item()
            if loss_perceptual.item() > 0:
                total_loss_perceptual += loss_perceptual.item()
            if loss_freq.item() > 0:
                total_loss_frequency += loss_freq.item()
            if loss_ssim_val.item() > 0:
                total_loss_ssim += loss_ssim_val.item()
            
            # æ—¥å¿—
            if self.global_step % self.config['train']['log_interval'] == 0:
                self.writer.add_scalar('train/loss', loss.item() * self.grad_accum_steps, self.global_step)
                self.writer.add_scalar('train/loss_fm', loss_fm.item(), self.global_step)
                if loss_perceptual.item() > 0:
                    self.writer.add_scalar('train/loss_perceptual', loss_perceptual.item(), self.global_step)
                if loss_freq.item() > 0:
                    self.writer.add_scalar('train/loss_frequency', loss_freq.item(), self.global_step)
                if loss_ssim_val.item() > 0:
                    self.writer.add_scalar('train/loss_ssim', loss_ssim_val.item(), self.global_step)
                self.writer.add_scalar('train/lr', self.optimizer.param_groups[0]['lr'], self.global_step)
            
            postfix_dict = {
                'loss': f"{loss.item() * self.grad_accum_steps:.4f}",
                'fm': f"{loss_fm.item():.4f}",
            }
            if loss_freq.item() > 0:
                postfix_dict['freq'] = f"{loss_freq.item():.4f}"
            if loss_ssim_val.item() > 0:
                postfix_dict['ssim'] = f"{loss_ssim_val.item():.4f}"
            postfix_dict['lr'] = f"{self.optimizer.param_groups[0]['lr']:.6f}"
            
            pbar.set_postfix(postfix_dict)
            
            self.global_step += 1
        
        avg_loss = total_loss / len(self.train_loader)
        avg_loss_fm = total_loss_fm / len(self.train_loader)
        
        return avg_loss, avg_loss_fm
    
    @torch.no_grad()
    def validate(self, epoch):
        """éªŒè¯"""
        self.model.eval()
        
        total_loss = 0
        total_loss_fm = 0
        total_loss_frequency = 0
        total_loss_ssim = 0
        
        for sim_images, real_images, _ in tqdm(self.val_loader, desc="Validation"):
            sim_images = sim_images.to(self.device)
            real_images = real_images.to(self.device)
            
            # Flow Matching Loss
            loss_fm = self.model.compute_loss(sim_images, real_images)
            
            # è·å–é¢„æµ‹ç”¨äºç»“æ„loss
            batch_size = sim_images.shape[0]
            t_mid = torch.ones(batch_size, device=self.device) * 0.5
            noise = torch.randn_like(real_images)
            x_mid = 0.5 * noise + 0.5 * real_images
            v_pred = self.model(x_mid, t_mid, sim_images)
            predicted = x_mid + v_pred * 0.5
            
            # é¢‘åŸŸLoss
            loss_freq = torch.tensor(0.0, device=self.device)
            if self.config['loss'].get('use_frequency', False):
                loss_freq = frequency_domain_loss(predicted, real_images)
            
            # SSIM Loss
            loss_ssim_val = torch.tensor(0.0, device=self.device)
            if self.config['loss'].get('use_ssim', False):
                loss_ssim_val = ssim_loss(predicted, real_images)
            
            # æ€»lossï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒçš„æƒé‡ï¼‰
            loss = (
                loss_fm + 
                self.config['loss'].get('frequency_weight', 0.1) * loss_freq +
                self.config['loss'].get('ssim_weight', 0.3) * loss_ssim_val
            )
            
            total_loss += loss.item()
            total_loss_fm += loss_fm.item()
            if loss_freq.item() > 0:
                total_loss_frequency += loss_freq.item()
            if loss_ssim_val.item() > 0:
                total_loss_ssim += loss_ssim_val.item()
        
        avg_loss = total_loss / len(self.val_loader)
        avg_loss_fm = total_loss_fm / len(self.val_loader)
        
        # è®°å½•
        self.writer.add_scalar('val/loss', avg_loss, epoch)
        self.writer.add_scalar('val/loss_fm', avg_loss_fm, epoch)
        if total_loss_frequency > 0:
            self.writer.add_scalar('val/loss_frequency', total_loss_frequency / len(self.val_loader), epoch)
        if total_loss_ssim > 0:
            self.writer.add_scalar('val/loss_ssim', total_loss_ssim / len(self.val_loader), epoch)
        
        return avg_loss
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        
        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # ä¿å­˜æœ€æ–°
        checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        print(f"ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        # ä¿å­˜æœ€ä½³
        if is_best:
            best_path = self.checkpoint_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
        
        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
        keep_n = self.config['train']['keep_last_n_checkpoints']
        if keep_n > 0:
            # æŒ‰epochæ•°å­—æ’åºï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²æ’åº
            checkpoints = sorted(
                self.checkpoint_dir.glob("checkpoint_epoch_*.pth"),
                key=lambda p: int(p.stem.split('_')[-1])  # æå–epochæ•°å­—
            )
            # åˆ é™¤æ—§çš„ï¼Œä¿ç•™æœ€è¿‘Nä¸ª
            for ckpt in checkpoints[:-keep_n]:
                ckpt.unlink()
                print(f"åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {ckpt.name}")
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        print(f"åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if checkpoint['scheduler_state_dict'] and self.scheduler:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.start_epoch = checkpoint['epoch'] + 1
        self.global_step = checkpoint['global_step']
        self.best_val_loss = checkpoint['best_val_loss']
        
        if self.scaler and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\nå¼€å§‹è®­ç»ƒ...")
        
        # ğŸ”§ ç»Ÿä¸€ï¼šç¡®ä¿è®­ç»ƒå®Œæ•´çš„num_epochsä¸ªepochï¼ˆä»1åˆ°num_epochsï¼‰
        for epoch in range(self.start_epoch, self.config['train']['num_epochs'] + 1):
            # è®­ç»ƒ
            train_loss, train_loss_fm = self.train_one_epoch(epoch)
            
            # éªŒè¯
            val_loss = self.validate(epoch)
            
            # è®°å½•epochçº§åˆ«çš„æŒ‡æ ‡
            self.writer.add_scalar('epoch/train_loss', train_loss, epoch)
            self.writer.add_scalar('epoch/val_loss', val_loss, epoch)
            
            print(f"\nEpoch {epoch}:")
            print(f"  Train Loss: {train_loss:.6f} (FM: {train_loss_fm:.6f})")
            print(f"  Val Loss: {val_loss:.6f}")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss
                # é—®é¢˜1ä¿®å¤ï¼šç«‹å³ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œä¸ä¾èµ–save_interval
                self.save_checkpoint(epoch, is_best=True)
                print(f"  âœ“ æ–°çš„æœ€ä½³æ¨¡å‹ï¼Val Loss: {self.best_val_loss:.6f}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆéæœ€ä½³æ¨¡å‹æ—¶ï¼‰
            if (epoch + 1) % self.config['train']['save_interval'] == 0:
                if not is_best:  # å¦‚æœä¸æ˜¯æœ€ä½³æ¨¡å‹ï¼Œæ‰ä¿å­˜å®šæœŸæ£€æŸ¥ç‚¹
                    self.save_checkpoint(epoch, is_best=False)
            
            # æ—©åœ
            if self.early_stopping:
                if self.early_stopping(val_loss):
                    print(f"\næ—©åœè§¦å‘ï¼æœ€ä½³Val Loss: {self.best_val_loss:.6f}")
                    # é—®é¢˜2ä¿®å¤ï¼šæ—©åœæ—¶ç¡®ä¿æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼Œç„¶åä¿å­˜æœ€ç»ˆæ¨¡å‹
                    best_model_path = self.checkpoint_dir / "best_model.pth"
                    if best_model_path.exists():
                        print(f"  æœ€ä½³æ¨¡å‹å·²ä¿å­˜åœ¨: {best_model_path}")
                        # åŠ è½½æœ€ä½³æ¨¡å‹çš„çŠ¶æ€ç”¨äºä¿å­˜final_model
                        best_checkpoint = torch.load(best_model_path, map_location=self.device, weights_only=False)
                        # ä¿å­˜æ—©åœæ—¶çš„æœ€ç»ˆæ¨¡å‹ï¼ˆä½¿ç”¨æœ€ä½³æ¨¡å‹çš„çŠ¶æ€ï¼‰
                        final_path = self.checkpoint_dir / "final_model.pth"
                        final_checkpoint = {
                            'epoch': best_checkpoint['epoch'],  # ä½¿ç”¨æœ€ä½³æ¨¡å‹çš„epoch
                            'global_step': best_checkpoint['global_step'],
                            'model_state_dict': best_checkpoint['model_state_dict'],  # ä½¿ç”¨æœ€ä½³æ¨¡å‹çš„æƒé‡
                            'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],
                            'scheduler_state_dict': best_checkpoint['scheduler_state_dict'],
                            'best_val_loss': self.best_val_loss,
                            'config': self.config,
                            'early_stopped': True
                        }
                        if self.scaler and 'scaler_state_dict' in best_checkpoint:
                            final_checkpoint['scaler_state_dict'] = best_checkpoint['scaler_state_dict']
                        torch.save(final_checkpoint, final_path)
                        print(f"  ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆæ—©åœï¼Œä½¿ç”¨æœ€ä½³æ¨¡å‹æƒé‡ï¼‰: {final_path}")
                    else:
                        # å¦‚æœæœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œä¿å­˜å½“å‰æ¨¡å‹
                        print(f"  è­¦å‘Šï¼šæœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¿å­˜å½“å‰æ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹")
                        final_path = self.checkpoint_dir / "final_model.pth"
                        checkpoint = {
                            'epoch': epoch,
                            'global_step': self.global_step,
                            'model_state_dict': self.model.state_dict(),
                            'optimizer_state_dict': self.optimizer.state_dict(),
                            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
                            'best_val_loss': self.best_val_loss,
                            'config': self.config,
                            'early_stopped': True
                        }
                        if self.scaler:
                            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
                        torch.save(checkpoint, final_path)
                        print(f"  ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆæ—©åœï¼‰: {final_path}")
                    break
        
        # æ­£å¸¸è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
        else:
            # é—®é¢˜2ä¿®å¤ï¼šæ­£å¸¸è®­ç»ƒç»“æŸæ—¶ï¼Œä¹Ÿä½¿ç”¨æœ€ä½³æ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹
            final_path = self.checkpoint_dir / "final_model.pth"
            best_model_path = self.checkpoint_dir / "best_model.pth"
            if best_model_path.exists():
                print(f"\nè®­ç»ƒå®Œæˆï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹...")
                best_checkpoint = torch.load(best_model_path, map_location=self.device, weights_only=False)
                final_checkpoint = {
                    'epoch': best_checkpoint['epoch'],
                    'global_step': best_checkpoint['global_step'],
                    'model_state_dict': best_checkpoint['model_state_dict'],
                    'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],
                    'scheduler_state_dict': best_checkpoint['scheduler_state_dict'],
                    'best_val_loss': self.best_val_loss,
                    'config': self.config,
                    'early_stopped': False
                }
                if self.scaler and 'scaler_state_dict' in best_checkpoint:
                    final_checkpoint['scaler_state_dict'] = best_checkpoint['scaler_state_dict']
                torch.save(final_checkpoint, final_path)
                print(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆè®­ç»ƒå®Œæˆï¼Œä½¿ç”¨æœ€ä½³æ¨¡å‹æƒé‡ï¼‰: {final_path}")
            else:
                # å¦‚æœæœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œä¿å­˜å½“å‰æ¨¡å‹
                print(f"è­¦å‘Šï¼šæœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¿å­˜å½“å‰æ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹")
                checkpoint = {
                    'epoch': self.config['train']['num_epochs'] - 1,
                    'global_step': self.global_step,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
                    'best_val_loss': self.best_val_loss,
                    'config': self.config,
                    'early_stopped': False
                }
                if self.scaler:
                    checkpoint['scaler_state_dict'] = self.scaler.state_dict()
                torch.save(checkpoint, final_path)
                print(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆè®­ç»ƒå®Œæˆï¼‰: {final_path}")
        
        print("\nè®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³Val Loss: {self.best_val_loss:.6f}")
        print(f"æœ€ä½³æ¨¡å‹: {self.checkpoint_dir / 'best_model.pth'}")
        print(f"æœ€ç»ˆæ¨¡å‹: {self.checkpoint_dir / 'final_model.pth'}")
        self.writer.close()


def main():
    parser = argparse.ArgumentParser(description="Train Sim2Real Flow Matching V2")
    parser.add_argument('--config', type=str, default='config_v2.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    
    trainer = Trainer(args.config)
    trainer.train()


if __name__ == "__main__":
    main()

