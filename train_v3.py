"""
è®­ç»ƒè„šæœ¬ V3 - çº¯å›¾åƒå¯¹çš„Sim2Real Flow Matching
å¢å¼ºåŠŸèƒ½ï¼š
1. è®­ç»ƒæ—¶æ”¶é›†PSNRå†å²ï¼Œç¡®å®šåŠ¨æ€èŒƒå›´
2. è®­ç»ƒç»“æŸåè‡ªåŠ¨åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
3. è®¡ç®—ç»¼åˆè¯„åˆ†
"""
import os
import sys
import argparse
from pathlib import Path
import yaml
import random
import numpy as np
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from models_v2 import Sim2RealFlowModel
from models_v2.perceptual_loss import PerceptualLoss
from utils_v2 import RDPairDataset
from utils_v2.losses import frequency_domain_loss, ssim_loss
from utils_v3 import ImageQualityMetrics, ModelEvaluator
from utils_v3.discriminator import DopplerClutterDiscriminator, doppler_clutter_gan_loss
from torch.utils.data import DataLoader
import torchvision.transforms as transforms


class EarlyStopping:
    """æ—©åœæœºåˆ¶
    
    å¯¹äºlossæŒ‡æ ‡ï¼ˆè¶Šå°è¶Šå¥½ï¼‰ï¼š
    - å¦‚æœ score < best_score - min_deltaï¼šè®¤ä¸ºæœ‰æ”¹å–„ï¼Œæ›´æ–°best_scoreå¹¶é‡ç½®counter
    - å¦‚æœ score >= best_score - min_deltaï¼šè®¤ä¸ºæ²¡æœ‰æ”¹å–„æˆ–æ”¹å–„ä¸è¶³ï¼Œcounterå¢åŠ 
    - å½“counter >= patienceæ—¶ï¼Œè§¦å‘æ—©åœ
    """
    def __init__(self, patience=20, min_delta=0.0001, monitor='val_loss'):
        self.patience = patience
        self.min_delta = min_delta
        self.monitor = monitor
        self.counter = 0
        self.best_score = None
        self.early_stop = False
    
    def __call__(self, score):
        """
        Args:
            score: å½“å‰æŒ‡æ ‡å€¼ï¼ˆå¯¹äºlossï¼Œè¶Šå°è¶Šå¥½ï¼‰
        Returns:
            bool: æ˜¯å¦è§¦å‘æ—©åœ
        """
        if self.best_score is None:
            # ç¬¬ä¸€ä¸ªepochï¼Œåˆå§‹åŒ–best_score
            self.best_score = score
        elif score > self.best_score - self.min_delta:
            # æ²¡æœ‰æ”¹å–„æˆ–æ”¹å–„ä¸è¶³min_deltaï¼ˆå¯¹äºlossï¼šscoreè¶Šå¤§è¡¨ç¤ºè¶Šå·®ï¼‰
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            # æœ‰æ”¹å–„ï¼ˆscore < best_score - min_deltaï¼Œå¯¹äºlossè¡¨ç¤ºé™ä½äº†è‡³å°‘min_deltaï¼‰
            self.best_score = score
            self.counter = 0
        
        return self.early_stop


class TrainerV3:
    def __init__(self, config_path):
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.setup_environment()
        self.setup_paths()
        self.setup_model()
        self.setup_data()
        self.setup_optimizer()
        self.setup_training()
        
        # PSNRå†å²è®°å½•ï¼ˆç”¨äºç¡®å®šåŠ¨æ€èŒƒå›´ï¼‰
        self.val_psnr_history = []
        
        # åˆ¤åˆ«å™¨ï¼ˆå¦‚æœå¯ç”¨GANï¼‰
        self.discriminator = None
        self.use_gan = self.config.get('gan', {}).get('enabled', False)
        if self.use_gan:
            self.setup_discriminator()
        
        print("="*60)
        print("è®­ç»ƒé…ç½® V3:")
        print(f"  æ¨¡å‹: Sim2RealFlowModel V2")
        print(f"  å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"  è®­ç»ƒé›†: {len(self.train_loader.dataset)}")
        print(f"  éªŒè¯é›†: {len(self.val_loader.dataset)}")
        if hasattr(self, 'test_loader'):
            print(f"  æµ‹è¯•é›†: {len(self.test_loader.dataset)}")
        print(f"  æ‰¹å¤§å°: {self.config['train']['batch_size']}")
        print(f"  æ¢¯åº¦ç´¯ç§¯: {self.config['train']['gradient_accumulation_steps']}")
        print(f"  å®é™…æ‰¹å¤§å°: {self.config['train']['batch_size'] * self.config['train']['gradient_accumulation_steps']}")
        print(f"  å­¦ä¹ ç‡: {self.config['train']['learning_rate']}")
        print(f"  æ··åˆç²¾åº¦: {self.config['train']['mixed_precision']}")
        print(f"  æ„ŸçŸ¥æŸå¤±: {self.config['loss']['use_perceptual']} (æƒé‡={self.config['loss']['perceptual_weight']})")
        print(f"  è‡ªåŠ¨æµ‹è¯•: {self.config['train'].get('auto_test', False)}")
        if self.use_gan:
            print(f"  GAN: å¯ç”¨")
            gan_cfg = self.config.get('gan', {})
            print(f"    å¤šæ™®å‹’æƒé‡: {gan_cfg.get('doppler_weight', 0.75)}")
            print(f"    åœ°æ‚æ³¢æƒé‡: {gan_cfg.get('clutter_weight', 0.25)}")
            print(f"    GANæŸå¤±æƒé‡: {gan_cfg.get('weights', {}).get('gan', 0.3)}")
        else:
            print(f"  GAN: æœªå¯ç”¨")
        print("="*60)
    
    def setup_environment(self):
        """è®¾ç½®ç¯å¢ƒ"""
        # éšæœºç§å­
        seed = self.config.get('misc', {}).get('seed', 42)
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        
        # CuDNN
        if self.config.get('misc', {}).get('cudnn_benchmark', True):
            torch.backends.cudnn.benchmark = True
        if self.config.get('misc', {}).get('deterministic', False):
            torch.backends.cudnn.deterministic = True
        
        # è®¾å¤‡
        self.device = torch.device(self.config['train']['device'])
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def setup_paths(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        self.output_dir = Path(self.config['paths']['output_dir'])
        self.log_dir = Path(self.config['paths']['log_dir'])
        self.checkpoint_dir = Path(self.config['paths']['checkpoint_dir'])
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir=str(self.log_dir))
    
    def setup_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        model_cfg = self.config['model']
        
        self.model = Sim2RealFlowModel(
            base_channels=model_cfg['base_channels'],
            channel_mult=tuple(model_cfg['channel_mult']),
            time_embed_dim=model_cfg['time_embed_dim'],
            num_res_blocks=model_cfg['num_res_blocks'],
            attention_levels=tuple(model_cfg['attention_levels']),
            dropout=model_cfg['dropout']
        ).to(self.device)
        
        # Perceptual Loss
        if self.config['loss']['use_perceptual']:
            self.perceptual_criterion = PerceptualLoss(
                feature_layers=tuple(self.config['loss']['perceptual_layers'])
            ).to(self.device)
        else:
            self.perceptual_criterion = None
        
        # æ¢å¤è®­ç»ƒ
        self.start_epoch = 1  # ğŸ”§ ç»Ÿä¸€ï¼šepochä»1å¼€å§‹
        self.global_step = 0
        self.best_val_loss = float('inf')
        
        if self.config['resume']['checkpoint']:
            self.load_checkpoint(self.config['resume']['checkpoint'])
    
    def setup_data(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        # å›¾åƒå˜æ¢
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[self.config['data']['normalize_mean']],
                std=[self.config['data']['normalize_std']]
            )
        ])
        
        # è®­ç»ƒé›†
        train_dataset = RDPairDataset(
            data_root=self.config['data']['train_root'],
            transform=transform,
            augment=self.config['data']['augment']
        )
        
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['train']['batch_size'],
            shuffle=True,
            num_workers=self.config['train']['num_workers'],
            pin_memory=True,
            drop_last=True
        )
        
        # éªŒè¯é›†
        val_dataset = RDPairDataset(
            data_root=self.config['data']['val_root'],
            transform=transform,
            augment=False
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['train']['batch_size'],
            shuffle=False,
            num_workers=self.config['train']['num_workers'],
            pin_memory=True,
            drop_last=False
        )
        
        # æµ‹è¯•é›†ï¼ˆå¦‚æœå¯ç”¨è‡ªåŠ¨æµ‹è¯•ï¼‰
        if self.config['train'].get('auto_test', False):
            test_dataset = RDPairDataset(
                data_root=self.config['data']['test_root'],
                transform=transform,
                augment=False
            )
            
            self.test_loader = DataLoader(
                test_dataset,
                batch_size=1,  # æµ‹è¯•æ—¶é€å¼ å¤„ç†
                shuffle=False,
                num_workers=0,
                pin_memory=True,
                drop_last=False
            )
    
    def setup_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
        # ä¼˜åŒ–å™¨
        if self.config['train']['optimizer'] == 'adamw':
            self.optimizer = torch.optim.AdamW(
                self.model.parameters(),
                lr=self.config['train']['learning_rate'],
                betas=tuple(self.config['train']['betas']),
                weight_decay=self.config['train']['weight_decay']
            )
        else:
            self.optimizer = torch.optim.Adam(
                self.model.parameters(),
                lr=self.config['train']['learning_rate'],
                betas=tuple(self.config['train']['betas'])
            )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config['train']['lr_scheduler'] == 'cosine':
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['train']['num_epochs'] - self.config['train']['lr_warmup_epochs'],
                eta_min=self.config['train']['lr_min']
            )
        elif self.config['train']['lr_scheduler'] == 'step':
            self.scheduler = torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=30,
                gamma=0.1
            )
        elif self.config['train']['lr_scheduler'] == 'plateau':
            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode=self.config['train'].get('plateau_mode', 'min'),
                factor=self.config['train'].get('plateau_factor', 0.5),
                patience=self.config['train'].get('plateau_patience', 10),
                min_lr=self.config['train'].get('plateau_min_lr', 1e-6)
            )
        else:
            self.scheduler = None
        
        # æ··åˆç²¾åº¦
        self.scaler = torch.amp.GradScaler('cuda') if self.config['train']['mixed_precision'] else None
    
    def setup_training(self):
        """è®¾ç½®è®­ç»ƒç›¸å…³"""
        # æ—©åœ
        if self.config['train']['early_stopping']['enabled']:
            self.early_stopping = EarlyStopping(
                patience=self.config['train']['early_stopping']['patience'],
                min_delta=self.config['train']['early_stopping']['min_delta'],
                monitor=self.config['train']['early_stopping']['monitor']
            )
        else:
            self.early_stopping = None
        
        # æ¢¯åº¦ç´¯ç§¯
        self.grad_accum_steps = self.config['train']['gradient_accumulation_steps']
    
    def setup_discriminator(self):
        """è®¾ç½®åˆ¤åˆ«å™¨"""
        gan_cfg = self.config.get('gan', {})
        
        # åˆ›å»ºåˆ¤åˆ«å™¨
        self.discriminator = DopplerClutterDiscriminator(
            doppler_weight=gan_cfg.get('doppler_weight', 0.75),
            clutter_weight=gan_cfg.get('clutter_weight', 0.25)
        ).to(self.device)
        
        # åˆ¤åˆ«å™¨ä¸éœ€è¦è®­ç»ƒï¼ˆåªæ˜¯ç‰¹å¾æå–å’Œå·®åˆ«è®¡ç®—ï¼‰
        # æˆ–è€…å¯ä»¥é€‰æ‹©æ€§åœ°è®­ç»ƒåˆ¤åˆ«å™¨æ¥æ›´å¥½åœ°æå–ç‰¹å¾
        if gan_cfg.get('train_discriminator', False):
            # å¦‚æœé€‰æ‹©è®­ç»ƒåˆ¤åˆ«å™¨ï¼Œéœ€è¦ä¼˜åŒ–å™¨
            self.discriminator_optimizer = torch.optim.Adam(
                self.discriminator.parameters(),
                lr=gan_cfg.get('lr_discriminator', 2e-4)
            )
        else:
            # å†»ç»“åˆ¤åˆ«å™¨å‚æ•°ï¼ˆåªç”¨äºç‰¹å¾æå–ï¼‰
            for param in self.discriminator.parameters():
                param.requires_grad = False
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆä¸è®­ç»ƒï¼‰
            self.discriminator.eval()
        
        print(f"åˆ¤åˆ«å™¨å·²åˆ›å»ºï¼ˆå¤šæ™®å‹’æƒé‡: {gan_cfg.get('doppler_weight', 0.75)}, "
              f"åœ°æ‚æ³¢æƒé‡: {gan_cfg.get('clutter_weight', 0.25)}ï¼‰")
        if gan_cfg.get('train_discriminator', False):
            print("  åˆ¤åˆ«å™¨å°†å‚ä¸è®­ç»ƒ")
        else:
            print("  åˆ¤åˆ«å™¨ä»…ç”¨äºç‰¹å¾æå–ï¼ˆä¸è®­ç»ƒï¼‰")
    
    def train_one_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0
        total_loss_fm = 0
        total_loss_perceptual = 0
        total_loss_frequency = 0
        total_loss_ssim = 0
        total_loss_gan = 0
        total_loss_gan_doppler = 0
        total_loss_gan_clutter = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}/{self.config['train']['num_epochs']}")
        
        for batch_idx, (sim_images, real_images, _) in enumerate(pbar):
            sim_images = sim_images.to(self.device)
            real_images = real_images.to(self.device)
            
            # æ··åˆç²¾åº¦
            with torch.amp.autocast('cuda', enabled=self.config['train']['mixed_precision']):
                # Flow Matching Loss
                loss_fm = self.model.compute_loss(sim_images, real_images)
                
                # è·å–ä¸€ä¸ªé¢„æµ‹æ ·æœ¬ç”¨äºç»“æ„lossè®¡ç®—
                batch_size = sim_images.shape[0]
                t_mid = torch.ones(batch_size, device=self.device) * 0.5
                noise = torch.randn_like(real_images)
                x_mid = 0.5 * noise + 0.5 * real_images
                v_pred = self.model(x_mid, t_mid, sim_images)
                predicted = x_mid + v_pred * 0.5
                
                # Perceptual Lossï¼ˆç®€åŒ–ç‰ˆï¼‰
                loss_perceptual = torch.tensor(0.0, device=self.device)
                if (self.perceptual_criterion is not None and 
                    self.global_step % self.config['loss']['perceptual_interval'] == 0):
                    loss_perceptual = self.perceptual_criterion(predicted, real_images)
                
                # é¢‘åŸŸLoss
                loss_freq = torch.tensor(0.0, device=self.device)
                if self.config['loss'].get('use_frequency', False):
                    loss_freq = frequency_domain_loss(predicted, real_images)
                
                # SSIM Loss
                loss_ssim_val = torch.tensor(0.0, device=self.device)
                if self.config['loss'].get('use_ssim', False):
                    loss_ssim_val = ssim_loss(predicted, real_images)
                
                # GAN Lossï¼ˆå¤šæ™®å‹’+åœ°æ‚æ³¢ï¼Œå¦‚æœå¯ç”¨ï¼‰
                loss_gan = torch.tensor(0.0, device=self.device)
                loss_gan_doppler = torch.tensor(0.0, device=self.device)
                loss_gan_clutter = torch.tensor(0.0, device=self.device)
                if self.use_gan and self.discriminator is not None:
                    # åˆ¤åˆ«å™¨è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå¦‚æœä¸éœ€è¦è®­ç»ƒï¼‰
                    if not self.config.get('gan', {}).get('train_discriminator', False):
                        self.discriminator.eval()
                    # è®¡ç®—GANæŸå¤±
                    loss_gan_val, gan_diffs = doppler_clutter_gan_loss(
                        self.discriminator,
                        real_images,
                        predicted
                    )
                    loss_gan = loss_gan_val
                    loss_gan_doppler = gan_diffs['doppler_diff']
                    loss_gan_clutter = gan_diffs['clutter_diff']
                
                # æ€»Loss
                gan_weight = self.config.get('gan', {}).get('weights', {}).get('gan', 0.3)
                loss = (
                    loss_fm + 
                    self.config['loss']['perceptual_weight'] * loss_perceptual +
                    self.config['loss'].get('frequency_weight', 0.1) * loss_freq +
                    self.config['loss'].get('ssim_weight', 0.3) * loss_ssim_val +
                    gan_weight * loss_gan
                )
                loss = loss / self.grad_accum_steps
            
            # åå‘ä¼ æ’­
            if self.scaler:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()
            
            # æ¢¯åº¦ç´¯ç§¯
            if (batch_idx + 1) % self.grad_accum_steps == 0:
                # æ¢¯åº¦è£å‰ª
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config['train']['max_grad_norm']
                )
                
                # æ›´æ–°å‚æ•°
                if self.scaler:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()
                
                self.optimizer.zero_grad()
            
            # ç»Ÿè®¡
            total_loss += loss.item() * self.grad_accum_steps
            total_loss_fm += loss_fm.item()
            if loss_perceptual.item() > 0:
                total_loss_perceptual += loss_perceptual.item()
            if loss_freq.item() > 0:
                total_loss_frequency += loss_freq.item()
            if loss_ssim_val.item() > 0:
                total_loss_ssim += loss_ssim_val.item()
            if loss_gan.item() > 0:
                total_loss_gan += loss_gan.item()
                total_loss_gan_doppler += loss_gan_doppler.item()
                total_loss_gan_clutter += loss_gan_clutter.item()
            
            # æ—¥å¿—
            if self.global_step % self.config['train']['log_interval'] == 0:
                self.writer.add_scalar('train/loss', loss.item() * self.grad_accum_steps, self.global_step)
                self.writer.add_scalar('train/loss_fm', loss_fm.item(), self.global_step)
                if loss_perceptual.item() > 0:
                    self.writer.add_scalar('train/loss_perceptual', loss_perceptual.item(), self.global_step)
                if loss_freq.item() > 0:
                    self.writer.add_scalar('train/loss_frequency', loss_freq.item(), self.global_step)
                if loss_ssim_val.item() > 0:
                    self.writer.add_scalar('train/loss_ssim', loss_ssim_val.item(), self.global_step)
                if loss_gan.item() > 0:
                    self.writer.add_scalar('train/loss_gan', loss_gan.item(), self.global_step)
                    self.writer.add_scalar('train/loss_gan_doppler', loss_gan_doppler.item(), self.global_step)
                    self.writer.add_scalar('train/loss_gan_clutter', loss_gan_clutter.item(), self.global_step)
                self.writer.add_scalar('train/lr', self.optimizer.param_groups[0]['lr'], self.global_step)
            
            postfix_dict = {
                'loss': f"{loss.item() * self.grad_accum_steps:.4f}",
                'fm': f"{loss_fm.item():.4f}",
            }
            if loss_freq.item() > 0:
                postfix_dict['freq'] = f"{loss_freq.item():.4f}"
            if loss_ssim_val.item() > 0:
                postfix_dict['ssim'] = f"{loss_ssim_val.item():.4f}"
            if loss_gan.item() > 0:
                postfix_dict['gan'] = f"{loss_gan.item():.4f}"
                postfix_dict['gan_d'] = f"{loss_gan_doppler.item():.4f}"
            postfix_dict['lr'] = f"{self.optimizer.param_groups[0]['lr']:.6f}"
            
            pbar.set_postfix(postfix_dict)
            
            self.global_step += 1
        
        avg_loss = total_loss / len(self.train_loader)
        avg_loss_fm = total_loss_fm / len(self.train_loader)
        
        return avg_loss, avg_loss_fm
    
    @torch.no_grad()
    def validate(self, epoch):
        """éªŒè¯"""
        self.model.eval()
        
        total_loss = 0
        total_loss_fm = 0
        total_loss_frequency = 0
        total_loss_ssim = 0
        total_loss_gan = 0
        total_loss_gan_doppler = 0
        total_loss_gan_clutter = 0
        
        # ç”¨äºæ”¶é›†PSNR
        psnr_values = []
        
        for sim_images, real_images, _ in tqdm(self.val_loader, desc="Validation"):
            sim_images = sim_images.to(self.device)
            real_images = real_images.to(self.device)
            
            # Flow Matching Loss
            loss_fm = self.model.compute_loss(sim_images, real_images)
            
            # è·å–é¢„æµ‹ç”¨äºç»“æ„loss
            batch_size = sim_images.shape[0]
            t_mid = torch.ones(batch_size, device=self.device) * 0.5
            noise = torch.randn_like(real_images)
            x_mid = 0.5 * noise + 0.5 * real_images
            v_pred = self.model(x_mid, t_mid, sim_images)
            predicted = x_mid + v_pred * 0.5
            
            # è®¡ç®—PSNRï¼ˆç”¨äºç¡®å®šåŠ¨æ€èŒƒå›´ï¼‰
            for i in range(batch_size):
                pred_np = predicted[i].cpu().numpy().squeeze()
                real_np = real_images[i].cpu().numpy().squeeze()
                psnr_val = ImageQualityMetrics.psnr_score(pred_np, real_np, data_range=1.0)
                if np.isfinite(psnr_val):
                    psnr_values.append(psnr_val)
            
            # é¢‘åŸŸLoss
            loss_freq = torch.tensor(0.0, device=self.device)
            if self.config['loss'].get('use_frequency', False):
                loss_freq = frequency_domain_loss(predicted, real_images)
            
            # SSIM Loss
            loss_ssim_val = torch.tensor(0.0, device=self.device)
            if self.config['loss'].get('use_ssim', False):
                loss_ssim_val = ssim_loss(predicted, real_images)
            
            # GAN Lossï¼ˆéªŒè¯æ—¶ä¹Ÿè®¡ç®—ï¼Œä½†ä¸åå‘ä¼ æ’­ï¼‰
            loss_gan = torch.tensor(0.0, device=self.device)
            loss_gan_doppler = torch.tensor(0.0, device=self.device)
            loss_gan_clutter = torch.tensor(0.0, device=self.device)
            if self.use_gan and self.discriminator is not None:
                self.discriminator.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                with torch.no_grad():
                    loss_gan_val, gan_diffs = doppler_clutter_gan_loss(
                        self.discriminator,
                        real_images,
                        predicted
                    )
                    loss_gan = loss_gan_val
                    loss_gan_doppler = gan_diffs['doppler_diff']
                    loss_gan_clutter = gan_diffs['clutter_diff']
            
            # æ€»loss
            gan_weight = self.config.get('gan', {}).get('weights', {}).get('gan', 0.3)
            loss = (
                loss_fm + 
                self.config['loss'].get('frequency_weight', 0.1) * loss_freq +
                self.config['loss'].get('ssim_weight', 0.3) * loss_ssim_val +
                gan_weight * loss_gan
            )
            
            total_loss += loss.item()
            total_loss_fm += loss_fm.item()
            if loss_freq.item() > 0:
                total_loss_frequency += loss_freq.item()
            if loss_ssim_val.item() > 0:
                total_loss_ssim += loss_ssim_val.item()
            if loss_gan.item() > 0:
                total_loss_gan += loss_gan.item()
                total_loss_gan_doppler += loss_gan_doppler.item()
                total_loss_gan_clutter += loss_gan_clutter.item()
        
        # è®°å½•PSNRå†å²
        if psnr_values:
            avg_psnr = np.mean(psnr_values)
            self.val_psnr_history.extend(psnr_values)
            self.writer.add_scalar('val/psnr', avg_psnr, epoch)
        
        avg_loss = total_loss / len(self.val_loader)
        avg_loss_fm = total_loss_fm / len(self.val_loader)
        
        # è®°å½•
        self.writer.add_scalar('val/loss', avg_loss, epoch)
        self.writer.add_scalar('val/loss_fm', avg_loss_fm, epoch)
        if total_loss_frequency > 0:
            self.writer.add_scalar('val/loss_frequency', total_loss_frequency / len(self.val_loader), epoch)
        if total_loss_ssim > 0:
            self.writer.add_scalar('val/loss_ssim', total_loss_ssim / len(self.val_loader), epoch)
        if total_loss_gan > 0:
            self.writer.add_scalar('val/loss_gan', total_loss_gan / len(self.val_loader), epoch)
            self.writer.add_scalar('val/loss_gan_doppler', total_loss_gan_doppler / len(self.val_loader), epoch)
            self.writer.add_scalar('val/loss_gan_clutter', total_loss_gan_clutter / len(self.val_loader), epoch)
        
        return avg_loss
    
    def calculate_psnr_range(self):
        """æ ¹æ®PSNRå†å²è®¡ç®—åŠ¨æ€èŒƒå›´"""
        if not self.val_psnr_history:
            # æ²¡æœ‰å†å²æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼
            return {'min': 15.0, 'max': 35.0, 'mean': 25.0, 'std': 5.0}
        
        psnr_array = np.array(self.val_psnr_history)
        psnr_min = np.percentile(psnr_array, 5)  # P5ä½œä¸ºä¸‹ç•Œ
        psnr_max = np.percentile(psnr_array, 95)  # P95ä½œä¸ºä¸Šç•Œ
        
        # æ·»åŠ ä¸€äº›å®¹å·®
        psnr_min = max(psnr_min - 2, 10.0)  # æœ€ä½ä¸ä½äº10
        psnr_max = min(psnr_max + 3, 50.0)  # æœ€é«˜ä¸è¶…è¿‡50
        
        return {
            'min': float(psnr_min),
            'max': float(psnr_max),
            'mean': float(np.mean(psnr_array)),
            'std': float(np.std(psnr_array))
        }
    
    @torch.no_grad()
    def evaluate_on_testset(self):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å¹¶è®¡ç®—ç»¼åˆè¯„åˆ†"""
        if not hasattr(self, 'test_loader'):
            print("è­¦å‘Š: æœªåŠ è½½æµ‹è¯•é›†ï¼Œè·³è¿‡æµ‹è¯•è¯„ä¼°")
            return None
        
        print("\n" + "="*60)
        print("å¼€å§‹æµ‹è¯•é›†è¯„ä¼°...")
        print("="*60)
        
        self.model.eval()
        
        # è®¡ç®—PSNRèŒƒå›´
        psnr_range = self.calculate_psnr_range()
        evaluator = ModelEvaluator(psnr_range=psnr_range)
        
        # åå½’ä¸€åŒ–
        denormalize = transforms.Normalize(
            mean=[-self.config['data']['normalize_mean'] / self.config['data']['normalize_std']],
            std=[1.0 / self.config['data']['normalize_std']]
        )
        
        all_metrics = []
        ode_steps = self.config['inference']['ode_steps']
        ode_method = self.config['inference']['ode_method']
        
        print(f"  ODEæ­¥æ•°: {ode_steps}")
        print(f"  ODEæ–¹æ³•: {ode_method}")
        print(f"  PSNRèŒƒå›´: [{psnr_range['min']:.1f}, {psnr_range['max']:.1f}] dB")
        
        for sim_images, real_images, _ in tqdm(self.test_loader, desc="æµ‹è¯•é›†è¯„ä¼°"):
            sim_images = sim_images.to(self.device)
            real_images = real_images.to(self.device)
            
            # ç”Ÿæˆ
            generated = self.model.generate(sim_images, ode_steps=ode_steps, ode_method=ode_method)
            
            # è½¬æ¢ä¸ºnumpy
            generated_img = denormalize(generated.squeeze(0)).squeeze(0).cpu().numpy()
            real_img = denormalize(real_images.squeeze(0)).squeeze(0).cpu().numpy()
            generated_img = np.clip(generated_img, 0, 1)
            real_img = np.clip(real_img, 0, 1)
            
            # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
            metrics = ImageQualityMetrics.compute_all_metrics(
                generated_img, real_img, include_frequency=True
            )
            all_metrics.append(metrics)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        metric_keys = all_metrics[0].keys()
        avg_metrics = {}
        for key in metric_keys:
            values = [m[key] for m in all_metrics]
            finite_values = [v for v in values if np.isfinite(v)]
            if finite_values:
                avg_metrics[key] = np.mean(finite_values)
            else:
                avg_metrics[key] = 0.0
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        score = evaluator.calculate_score(avg_metrics)
        
        # æ‰“å°ç»“æœ
        print("\n" + evaluator.format_score_report(score, avg_metrics))
        
        # è®°å½•åˆ°TensorBoard
        self.writer.add_scalar('test/total_score', score['total_score'], 0)
        self.writer.add_scalar('test/freq_score', score['freq_score'], 0)
        self.writer.add_scalar('test/ssim_score', score['ssim_score'], 0)
        self.writer.add_scalar('test/psnr_score', score['psnr_score'], 0)
        self.writer.add_scalar('test/freq_correlation', score['freq_correlation'], 0)
        self.writer.add_scalar('test/ssim', score['ssim'], 0)
        self.writer.add_scalar('test/psnr', score['psnr'], 0)
        
        return {
            'avg_metrics': avg_metrics,
            'score': score,
            'psnr_range': psnr_range
        }
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        # è®¡ç®—PSNRèŒƒå›´
        psnr_range = self.calculate_psnr_range()
        
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'best_val_loss': self.best_val_loss,
            'config': self.config,
            'psnr_range': psnr_range  # ä¿å­˜PSNRèŒƒå›´
        }
        
        # ä¿å­˜åˆ¤åˆ«å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_gan and self.discriminator is not None:
            checkpoint['discriminator_state_dict'] = self.discriminator.state_dict()
            if hasattr(self, 'discriminator_optimizer'):
                checkpoint['discriminator_optimizer_state_dict'] = self.discriminator_optimizer.state_dict()
        
        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # ä¿å­˜æœ€æ–°
        checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        print(f"ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        # ä¿å­˜æœ€ä½³
        if is_best:
            best_path = self.checkpoint_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
        
        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
        keep_n = self.config['train']['keep_last_n_checkpoints']
        if keep_n > 0:
            checkpoints = sorted(
                self.checkpoint_dir.glob("checkpoint_epoch_*.pth"),
                key=lambda p: int(p.stem.split('_')[-1])
            )
            for ckpt in checkpoints[:-keep_n]:
                ckpt.unlink()
                print(f"åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {ckpt.name}")
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        print(f"åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if checkpoint['scheduler_state_dict'] and self.scheduler:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.start_epoch = checkpoint['epoch'] + 1
        self.global_step = checkpoint['global_step']
        self.best_val_loss = checkpoint['best_val_loss']
        
        # åŠ è½½åˆ¤åˆ«å™¨ï¼ˆå¦‚æœå¯ç”¨ä¸”æ£€æŸ¥ç‚¹ä¸­æœ‰ï¼‰
        if self.use_gan and self.discriminator is not None:
            if 'discriminator_state_dict' in checkpoint:
                self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
                print("åˆ¤åˆ«å™¨çŠ¶æ€å·²åŠ è½½")
            if hasattr(self, 'discriminator_optimizer') and 'discriminator_optimizer_state_dict' in checkpoint:
                self.discriminator_optimizer.load_state_dict(checkpoint['discriminator_optimizer_state_dict'])
        
        # åŠ è½½PSNRå†å²ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'psnr_range' in checkpoint:
            # å¯ä»¥ä»PSNRèŒƒå›´æ¨æ–­å†å²ï¼ˆä¸å®Œç¾ï¼Œä½†å¯ä»¥ä½¿ç”¨ï¼‰
            pass
        
        if self.scaler and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\nå¼€å§‹è®­ç»ƒ...")
        
        # ğŸ”§ ç»Ÿä¸€ï¼šç¡®ä¿è®­ç»ƒå®Œæ•´çš„num_epochsä¸ªepochï¼ˆä»1åˆ°num_epochsï¼‰
        for epoch in range(self.start_epoch, self.config['train']['num_epochs'] + 1):
            # è®­ç»ƒ
            train_loss, train_loss_fm = self.train_one_epoch(epoch)
            
            # éªŒè¯
            val_loss = self.validate(epoch)
            
            # è®°å½•epochçº§åˆ«çš„æŒ‡æ ‡
            self.writer.add_scalar('epoch/train_loss', train_loss, epoch)
            self.writer.add_scalar('epoch/val_loss', val_loss, epoch)
            
            print(f"\nEpoch {epoch}:")
            print(f"  Train Loss: {train_loss:.6f} (FM: {train_loss_fm:.6f})")
            print(f"  Val Loss: {val_loss:.6f}")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss
                # é—®é¢˜1ä¿®å¤ï¼šç«‹å³ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œä¸ä¾èµ–save_interval
                self.save_checkpoint(epoch, is_best=True)
                print(f"  âœ“ æ–°çš„æœ€ä½³æ¨¡å‹ï¼Val Loss: {self.best_val_loss:.6f}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆéæœ€ä½³æ¨¡å‹æ—¶ï¼‰
            if (epoch + 1) % self.config['train']['save_interval'] == 0:
                if not is_best:  # å¦‚æœä¸æ˜¯æœ€ä½³æ¨¡å‹ï¼Œæ‰ä¿å­˜å®šæœŸæ£€æŸ¥ç‚¹
                    self.save_checkpoint(epoch, is_best=False)
            
            # æ—©åœ
            if self.early_stopping:
                if self.early_stopping(val_loss):
                    print(f"\næ—©åœè§¦å‘ï¼æœ€ä½³Val Loss: {self.best_val_loss:.6f}")
                    # é—®é¢˜2ä¿®å¤ï¼šæ—©åœæ—¶ç¡®ä¿æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼Œç„¶åä¿å­˜æœ€ç»ˆæ¨¡å‹
                    best_model_path = self.checkpoint_dir / "best_model.pth"
                    if best_model_path.exists():
                        print(f"  æœ€ä½³æ¨¡å‹å·²ä¿å­˜åœ¨: {best_model_path}")
                        # åŠ è½½æœ€ä½³æ¨¡å‹çš„çŠ¶æ€ç”¨äºä¿å­˜final_model
                        best_checkpoint = torch.load(best_model_path, map_location=self.device, weights_only=False)
                        # ä¿å­˜æ—©åœæ—¶çš„æœ€ç»ˆæ¨¡å‹ï¼ˆä½¿ç”¨æœ€ä½³æ¨¡å‹çš„çŠ¶æ€ï¼‰
                        final_path = self.checkpoint_dir / "final_model.pth"
                        final_checkpoint = {
                            'epoch': best_checkpoint['epoch'],  # ä½¿ç”¨æœ€ä½³æ¨¡å‹çš„epoch
                            'global_step': best_checkpoint['global_step'],
                            'model_state_dict': best_checkpoint['model_state_dict'],  # ä½¿ç”¨æœ€ä½³æ¨¡å‹çš„æƒé‡
                            'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],
                            'scheduler_state_dict': best_checkpoint['scheduler_state_dict'],
                            'best_val_loss': self.best_val_loss,
                            'config': self.config,
                            'early_stopped': True,
                            'psnr_range': best_checkpoint.get('psnr_range', self.calculate_psnr_range())
                        }
                        # ä¿å­˜åˆ¤åˆ«å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        if self.use_gan and self.discriminator is not None:
                            if 'discriminator_state_dict' in best_checkpoint:
                                final_checkpoint['discriminator_state_dict'] = best_checkpoint['discriminator_state_dict']
                            if hasattr(self, 'discriminator_optimizer') and 'discriminator_optimizer_state_dict' in best_checkpoint:
                                final_checkpoint['discriminator_optimizer_state_dict'] = best_checkpoint['discriminator_optimizer_state_dict']
                        if self.scaler and 'scaler_state_dict' in best_checkpoint:
                            final_checkpoint['scaler_state_dict'] = best_checkpoint['scaler_state_dict']
                        torch.save(final_checkpoint, final_path)
                        print(f"  ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆæ—©åœï¼Œä½¿ç”¨æœ€ä½³æ¨¡å‹æƒé‡ï¼‰: {final_path}")
                    else:
                        # å¦‚æœæœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œä¿å­˜å½“å‰æ¨¡å‹
                        print(f"  è­¦å‘Šï¼šæœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¿å­˜å½“å‰æ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹")
                        final_path = self.checkpoint_dir / "final_model.pth"
                        checkpoint = {
                            'epoch': epoch,
                            'global_step': self.global_step,
                            'model_state_dict': self.model.state_dict(),
                            'optimizer_state_dict': self.optimizer.state_dict(),
                            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
                            'best_val_loss': self.best_val_loss,
                            'config': self.config,
                            'early_stopped': True,
                            'psnr_range': self.calculate_psnr_range()
                        }
                        if self.scaler:
                            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
                        torch.save(checkpoint, final_path)
                        print(f"  ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆæ—©åœï¼‰: {final_path}")
                    break
        
        # æ­£å¸¸è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
        else:
            # é—®é¢˜2ä¿®å¤ï¼šæ­£å¸¸è®­ç»ƒç»“æŸæ—¶ï¼Œä¹Ÿä½¿ç”¨æœ€ä½³æ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹
            final_path = self.checkpoint_dir / "final_model.pth"
            best_model_path = self.checkpoint_dir / "best_model.pth"
            if best_model_path.exists():
                print(f"\nè®­ç»ƒå®Œæˆï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹...")
                best_checkpoint = torch.load(best_model_path, map_location=self.device, weights_only=False)
                final_checkpoint = {
                    'epoch': best_checkpoint['epoch'],
                    'global_step': best_checkpoint['global_step'],
                    'model_state_dict': best_checkpoint['model_state_dict'],
                    'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],
                    'scheduler_state_dict': best_checkpoint['scheduler_state_dict'],
                    'best_val_loss': self.best_val_loss,
                    'config': self.config,
                    'early_stopped': False,
                    'psnr_range': best_checkpoint.get('psnr_range', self.calculate_psnr_range())
                }
                # ä¿å­˜åˆ¤åˆ«å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.use_gan and self.discriminator is not None:
                    if 'discriminator_state_dict' in best_checkpoint:
                        final_checkpoint['discriminator_state_dict'] = best_checkpoint['discriminator_state_dict']
                    if hasattr(self, 'discriminator_optimizer') and 'discriminator_optimizer_state_dict' in best_checkpoint:
                        final_checkpoint['discriminator_optimizer_state_dict'] = best_checkpoint['discriminator_optimizer_state_dict']
                if self.scaler and 'scaler_state_dict' in best_checkpoint:
                    final_checkpoint['scaler_state_dict'] = best_checkpoint['scaler_state_dict']
                torch.save(final_checkpoint, final_path)
                print(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆè®­ç»ƒå®Œæˆï¼Œä½¿ç”¨æœ€ä½³æ¨¡å‹æƒé‡ï¼‰: {final_path}")
            else:
                # å¦‚æœæœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œä¿å­˜å½“å‰æ¨¡å‹
                print(f"è­¦å‘Šï¼šæœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¿å­˜å½“å‰æ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹")
                checkpoint = {
                    'epoch': self.config['train']['num_epochs'] - 1,
                    'global_step': self.global_step,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
                    'best_val_loss': self.best_val_loss,
                    'config': self.config,
                    'early_stopped': False,
                    'psnr_range': self.calculate_psnr_range()
                }
                # ä¿å­˜åˆ¤åˆ«å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.use_gan and self.discriminator is not None:
                    checkpoint['discriminator_state_dict'] = self.discriminator.state_dict()
                    if hasattr(self, 'discriminator_optimizer'):
                        checkpoint['discriminator_optimizer_state_dict'] = self.discriminator_optimizer.state_dict()
                if self.scaler:
                    checkpoint['scaler_state_dict'] = self.scaler.state_dict()
                torch.save(checkpoint, final_path)
                print(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆè®­ç»ƒå®Œæˆï¼‰: {final_path}")
        
        print("\nè®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³Val Loss: {self.best_val_loss:.6f}")
        print(f"æœ€ä½³æ¨¡å‹: {self.checkpoint_dir / 'best_model.pth'}")
        print(f"æœ€ç»ˆæ¨¡å‹: {self.checkpoint_dir / 'final_model.pth'}")
        
        # è®¡ç®—PSNRèŒƒå›´
        psnr_range = self.calculate_psnr_range()
        print(f"\nPSNRç»Ÿè®¡:")
        print(f"  èŒƒå›´: [{psnr_range['min']:.1f}, {psnr_range['max']:.1f}] dB")
        print(f"  å‡å€¼: {psnr_range['mean']:.1f} dB")
        print(f"  æ ‡å‡†å·®: {psnr_range['std']:.1f} dB")
        
        # è‡ªåŠ¨æµ‹è¯•è¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config['train'].get('auto_test', False):
            test_result = self.evaluate_on_testset()
            if test_result:
                print(f"\næµ‹è¯•é›†ç»¼åˆè¯„åˆ†: {test_result['score']['total_score']:.2f} / 100")
                print(f"è¯„çº§: {test_result['score']['rating']}")
        
        self.writer.close()


def main():
    parser = argparse.ArgumentParser(description="Train Sim2Real Flow Matching V3")
    parser.add_argument('--config', type=str, default='config_v2.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    
    trainer = TrainerV3(args.config)
    trainer.train()


if __name__ == "__main__":
    main()

