# ğŸ“ æ’ä»¶å¼€å‘æŒ‡å—

æœ¬æŒ‡å—å°†æ•™ä½ å¦‚ä½•ä¸º Sim2Real WebUI å¼€å‘è‡ªå®šä¹‰æ¨ç†æ’ä»¶ã€‚

---

## ğŸ¯ **æ ¸å¿ƒæ¦‚å¿µ**

æ’ä»¶æ˜¯å®ç°äº† `InferenceInterface` æ¥å£çš„Pythonç±»ï¼ŒWebUIé€šè¿‡è¿™ä¸ªç»Ÿä¸€æ¥å£ä¸å„ç§Sim2Realæ¨¡å‹äº¤äº’ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦æ’ä»¶ï¼Ÿ

- âœ… **è§£è€¦**ï¼šæ¨¡å‹é€»è¾‘ä¸WebUIåˆ†ç¦»
- âœ… **æ‰©å±•**ï¼šæ–°å¢æ¨¡å‹æ— éœ€ä¿®æ”¹WebUIä»£ç 
- âœ… **å¤ç”¨**ï¼šåŒä¸€ä¸ªWebUIæ”¯æŒæ‰€æœ‰Sim2Realä»»åŠ¡

---

## ğŸš€ **å¿«é€Ÿå¼€å§‹**

### 1. å¤åˆ¶æ¨¡æ¿

```bash
cd webui/backend/plugins
cp plugin_template.py my_model_plugin.py
```

### 2. ä¿®æ”¹ç±»åå’Œé…ç½®

```python
class MyModelPlugin(InferenceInterface):
    """
    ä½ çš„æ¨¡å‹æ’ä»¶
    
    å¿…éœ€é…ç½®å‚æ•°:
        checkpoint_path: str - æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        device: str - è®¾å¤‡
    """
    
    def __init__(self, plugin_name: str, config: Dict[str, Any]):
        super().__init__(plugin_name, config)
        
        # æ·»åŠ ä½ çš„é…ç½®å‚æ•°
        self.your_param = config.get('your_param', default_value)
```

### 3. å®ç°å¿…éœ€æ–¹æ³•

ä½ å¿…é¡»å®ç°ä»¥ä¸‹5ä¸ªæŠ½è±¡æ–¹æ³•ï¼š

#### (1) `load_model()` - åŠ è½½æ¨¡å‹

```python
def load_model(self, checkpoint_path: str, device: str = 'cuda:0') -> bool:
    """åŠ è½½æ¨¡å‹åˆ°GPU/CPU"""
    try:
        # 1. åˆ›å»ºæ¨¡å‹
        self.model = YourModel()
        
        # 2. åŠ è½½æƒé‡
        checkpoint = torch.load(checkpoint_path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # 3. ç§»åŠ¨åˆ°è®¾å¤‡
        self.device = device
        self.model = self.model.to(device)
        self.model.eval()
        
        # 4. æ ‡è®°å·²åŠ è½½
        self.is_loaded = True
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        self.is_loaded = False
        return False
```

#### (2) `unload_model()` - å¸è½½æ¨¡å‹

```python
def unload_model(self) -> bool:
    """é‡Šæ”¾æ˜¾å­˜"""
    try:
        if self.model is not None:
            del self.model
            self.model = None
        
        torch.cuda.empty_cache()
        self.is_loaded = False
        
        print("âœ… æ¨¡å‹å¸è½½æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ å¸è½½å¤±è´¥: {e}")
        return False
```

#### (3) `inference()` - å•å¼ æ¨ç†

```python
@torch.no_grad()
def inference(
    self, 
    image_path: str,
    output_path: str,
    **kwargs
) -> Dict[str, Any]:
    """
    å•å¼ å›¾ç‰‡æ¨ç†
    
    Args:
        image_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
        output_path: è¾“å‡ºå›¾ç‰‡è·¯å¾„
        **kwargs: å‰ç«¯ä¼ å…¥çš„è‡ªå®šä¹‰å‚æ•°
    
    Returns:
        {
            'success': bool,
            'output_path': str,
            'inference_time': float,
            'message': str,
            'metadata': dict  # å¯é€‰
        }
    """
    if not self.is_loaded:
        return {'success': False, 'message': 'æ¨¡å‹æœªåŠ è½½'}
    
    try:
        start_time = time.time()
        
        # 1. åŠ è½½å›¾åƒ
        image = Image.open(image_path)
        
        # 2. é¢„å¤„ç†
        input_tensor = self.preprocess(image)
        
        # 3. æ¨ç†
        output_tensor = self.model(input_tensor)
        
        # 4. åå¤„ç†
        output_image = self.postprocess(output_tensor)
        
        # 5. ä¿å­˜
        output_image.save(output_path)
        
        inference_time = time.time() - start_time
        
        return {
            'success': True,
            'output_path': output_path,
            'inference_time': inference_time,
            'message': 'æ¨ç†æˆåŠŸ',
            'metadata': {
                # æ·»åŠ ä½ æƒ³è¿”å›çš„ä¿¡æ¯
                'param1': value1,
            }
        }
    except Exception as e:
        return {'success': False, 'message': f'æ¨ç†å¤±è´¥: {str(e)}'}
```

#### (4) `batch_inference()` - æ‰¹é‡æ¨ç†

```python
def batch_inference(
    self,
    image_paths: List[str],
    output_dir: str,
    **kwargs
) -> Dict[str, Any]:
    """æ‰¹é‡æ¨ç†"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    results = []
    succeeded = 0
    failed = 0
    start_time = time.time()
    
    for image_path in image_paths:
        # ç”Ÿæˆè¾“å‡ºè·¯å¾„
        output_path = output_dir / f"output_{Path(image_path).stem}.png"
        
        # è°ƒç”¨å•å¼ æ¨ç†
        result = self.inference(image_path, str(output_path), **kwargs)
        
        results.append({
            'input_path': image_path,
            'output_path': str(output_path) if result['success'] else None,
            'success': result['success'],
            'inference_time': result.get('inference_time', 0),
            'message': result['message']
        })
        
        if result['success']:
            succeeded += 1
        else:
            failed += 1
    
    return {
        'success': True,
        'total': len(image_paths),
        'succeeded': succeeded,
        'failed': failed,
        'results': results,
        'total_time': time.time() - start_time,
        'message': f'æ‰¹é‡æ¨ç†å®Œæˆ: {succeeded}/{len(image_paths)}'
    }
```

#### (5) `get_model_info()` - è·å–æ¨¡å‹ä¿¡æ¯

```python
def get_model_info(self) -> Dict[str, Any]:
    """è¿”å›æ¨¡å‹ä¿¡æ¯ï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰"""
    info = {
        'name': 'ä½ çš„æ¨¡å‹åç§°',
        'version': '1.0',
        'description': 'æ¨¡å‹æè¿°',
        'input_size': (512, 512),  # (H, W)
        'output_size': (512, 512),
        'supported_formats': ['.png', '.jpg', '.jpeg'],
        'default_params': {
            'ode_steps': 50,
            'param2': value2,
        },
        'custom_fields': {
            # è‡ªå®šä¹‰å­—æ®µ
        }
    }
    
    # å¦‚æœæ¨¡å‹å·²åŠ è½½ï¼Œæ·»åŠ å‚æ•°é‡ç­‰ä¿¡æ¯
    if self.is_loaded and self.model is not None:
        info['parameters'] = sum(p.numel() for p in self.model.parameters())
    
    return info
```

---

## ğŸ“š **å®Œæ•´ç¤ºä¾‹ï¼šFlow Matching V2æ’ä»¶**

å‚è€ƒ `plugins/flow_matching_v2_plugin.py`ï¼š

```python
class FlowMatchingV2Plugin(InferenceInterface):
    def __init__(self, plugin_name: str, config: Dict[str, Any]):
        super().__init__(plugin_name, config)
        self.base_channels = config.get('base_channels', 64)
        self.image_size = config.get('image_size', (512, 512))
        
    def load_model(self, checkpoint_path: str, device: str = 'cuda:0') -> bool:
        # ... å®ç°åŠ è½½é€»è¾‘
        
    def inference(self, image_path, output_path, **kwargs):
        # 1. åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('L')
        image_tensor = self.transform(image).unsqueeze(0).to(self.device)
        
        # 2. ODEæ±‚è§£ç”Ÿæˆå›¾åƒ
        output_tensor = self._ode_solver(
            sim_image=image_tensor,
            ode_steps=kwargs.get('ode_steps', 50)
        )
        
        # 3. ä¿å­˜
        output_image = self._tensor_to_image(output_tensor[0])
        output_image.save(output_path)
        
        # 4. è¿”å›ç»“æœ
        return {
            'success': True,
            'output_path': output_path,
            'inference_time': inference_time,
            'message': 'æ¨ç†æˆåŠŸ'
        }
```

---

## ğŸ”§ **é«˜çº§åŠŸèƒ½**

### 1. è‡ªå®šä¹‰é…ç½®éªŒè¯

é‡å†™ `validate_config()` æ–¹æ³•ï¼š

```python
def validate_config(self) -> bool:
    """éªŒè¯é…ç½®æ˜¯å¦åˆæ³•"""
    required_keys = ['checkpoint_path', 'device', 'your_param']
    for key in required_keys:
        if key not in self.config:
            print(f"ç¼ºå°‘é…ç½®å‚æ•°: {key}")
            return False
    return True
```

### 2. æ€§èƒ½æŒ‡æ ‡è·Ÿè¸ª

ä½¿ç”¨ `InferenceMetrics`ï¼š

```python
from webui.backend.core import InferenceMetrics

class MyPlugin(InferenceInterface):
    def __init__(self, plugin_name, config):
        super().__init__(plugin_name, config)
        self.metrics = InferenceMetrics()
    
    def inference(self, ...):
        # ... æ¨ç†é€»è¾‘ ...
        self.metrics.update(inference_time)
        
    def get_model_info(self):
        info = {...}
        if self.is_loaded:
            info['performance'] = self.metrics.get_stats()
        return info
```

### 3. è‡ªå®šä¹‰é¢„å¤„ç†/åå¤„ç†

é‡å†™å¯é€‰æ–¹æ³•ï¼š

```python
def preprocess(self, image: np.ndarray) -> torch.Tensor:
    """è‡ªå®šä¹‰é¢„å¤„ç†"""
    # 1. Resize
    image = cv2.resize(image, self.image_size)
    
    # 2. å½’ä¸€åŒ–
    image = image / 255.0
    
    # 3. è½¬ä¸ºTensor
    tensor = torch.from_numpy(image).float()
    return tensor.unsqueeze(0).to(self.device)

def postprocess(self, output: torch.Tensor) -> np.ndarray:
    """è‡ªå®šä¹‰åå¤„ç†"""
    # 1. å»å½’ä¸€åŒ–
    output = torch.clamp(output, 0, 1) * 255
    
    # 2. è½¬ä¸ºnumpy
    array = output.cpu().numpy().astype(np.uint8)
    
    return array
```

### 4. æ¥æ”¶å‰ç«¯è‡ªå®šä¹‰å‚æ•°

å‰ç«¯ä¼ å…¥çš„å‚æ•°é€šè¿‡ `**kwargs` æ¥æ”¶ï¼š

```python
def inference(self, image_path, output_path, **kwargs):
    # è·å–å‰ç«¯ä¼ å…¥çš„å‚æ•°
    temperature = kwargs.get('temperature', 1.0)
    guidance_scale = kwargs.get('guidance_scale', 7.5)
    
    # ä½¿ç”¨è¿™äº›å‚æ•°
    output = self.model(input, temperature=temperature, guidance_scale=guidance_scale)
```

å‰ç«¯åœ¨ `InferenceParams.vue` ä¸­æ·»åŠ å¯¹åº”çš„è¾“å…¥æ¡†ã€‚

---

## ğŸ“¦ **æ³¨å†Œæ’ä»¶**

### æ–¹å¼1ï¼šä»£ç æ³¨å†Œ

åœ¨ `backend/main.py` ä¸­ï¼š

```python
from plugins.my_model_plugin import MyModelPlugin

@app.on_event("startup")
async def startup_event():
    model_manager.register_plugin(
        plugin_name='my_model',
        plugin_class=MyModelPlugin,
        config={
            'checkpoint_path': '/path/to/checkpoint.pth',
            'device': 'cuda:0',
            'your_param': value
        }
    )
```

### æ–¹å¼2ï¼šåŠ¨æ€åŠ è½½

```python
model_manager.load_plugin_from_file(
    plugin_file='/path/to/my_model_plugin.py',
    plugin_class_name='MyModelPlugin',
    plugin_name='my_model',
    config={...}
)
```

---

## âœ… **æµ‹è¯•æ’ä»¶**

åœ¨æ’ä»¶æ–‡ä»¶æœ«å°¾æ·»åŠ æµ‹è¯•ä»£ç ï¼š

```python
if __name__ == "__main__":
    config = {
        'checkpoint_path': '/path/to/checkpoint.pth',
        'device': 'cuda:0',
    }
    
    plugin = MyModelPlugin('test', config)
    
    # æµ‹è¯•åŠ è½½
    if plugin.load_model(config['checkpoint_path']):
        print("\næ¨¡å‹ä¿¡æ¯:")
        import json
        print(json.dumps(plugin.get_model_info(), indent=2))
        
        # æµ‹è¯•æ¨ç†
        result = plugin.inference(
            image_path='/path/to/test.png',
            output_path='/tmp/output.png'
        )
        print(json.dumps(result, indent=2))
        
        plugin.unload_model()
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
cd webui/backend
python plugins/my_model_plugin.py
```

---

## ğŸ› **å¸¸è§é—®é¢˜**

### Q: å¦‚ä½•å¤„ç†ä¸åŒè¾“å…¥å°ºå¯¸ï¼Ÿ

**A**: åœ¨ `preprocess()` ä¸­ç»Ÿä¸€resizeï¼š

```python
def preprocess(self, image):
    if image.size != self.image_size:
        image = image.resize(self.image_size, Image.BILINEAR)
    return self.transform(image)
```

### Q: å¦‚ä½•æ”¯æŒæ‰¹é‡æ¨ç†åŠ é€Ÿï¼Ÿ

**A**: é‡å†™ `batch_inference()` ä½¿ç”¨æ‰¹é‡å¤„ç†ï¼š

```python
def batch_inference(self, image_paths, output_dir, **kwargs):
    # æ‰¹é‡åŠ è½½
    images = [self.preprocess(Image.open(p)) for p in image_paths]
    batch = torch.stack(images)
    
    # æ‰¹é‡æ¨ç†
    with torch.no_grad():
        outputs = self.model(batch)
    
    # æ‰¹é‡ä¿å­˜
    for i, output_path in enumerate(output_paths):
        self.postprocess(outputs[i]).save(output_path)
```

### Q: å¦‚ä½•å¤„ç†CUDA OOMï¼Ÿ

**A**: åœ¨ `inference()` ä¸­æ·»åŠ å¼‚å¸¸å¤„ç†ï¼š

```python
def inference(self, ...):
    try:
        output = self.model(input)
    except RuntimeError as e:
        if "out of memory" in str(e):
            torch.cuda.empty_cache()
            return {
                'success': False,
                'message': 'CUDAå†…å­˜ä¸è¶³ï¼Œè¯·é™ä½batch_sizeæˆ–å›¾åƒå°ºå¯¸'
            }
        raise e
```

---

## ğŸ“‹ **æ£€æŸ¥æ¸…å•**

å¼€å‘å®Œæˆåï¼Œç¡®è®¤ä»¥ä¸‹äº‹é¡¹ï¼š

- [ ] ç»§æ‰¿è‡ª `InferenceInterface`
- [ ] å®ç°æ‰€æœ‰5ä¸ªæŠ½è±¡æ–¹æ³•
- [ ] `load_model()` è®¾ç½® `self.is_loaded = True`
- [ ] `inference()` è¿”å›æ­£ç¡®æ ¼å¼çš„å­—å…¸
- [ ] `get_model_info()` è¿”å›å®Œæ•´ä¿¡æ¯
- [ ] é€šè¿‡å•å…ƒæµ‹è¯•
- [ ] åœ¨WebUIä¸­æµ‹è¯•æ¨ç†
- [ ] æ·»åŠ æ–‡æ¡£æ³¨é‡Š

---

## ğŸ‰ **å®Œæˆï¼**

ç°åœ¨ä½ çš„æ’ä»¶å·²ç»å¯ä»¥åœ¨WebUIä¸­ä½¿ç”¨äº†ï¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒ `plugins/flow_matching_v2_plugin.py` ç¤ºä¾‹ã€‚

