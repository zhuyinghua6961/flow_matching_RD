"""
V5 WGAN-GPè®­ç»ƒå™¨
åŸºäºV4è®­ç»ƒå™¨æ¶æ„ï¼Œé›†æˆWGAN-GPæ›¿ä»£ä¼ ç»ŸGAN
"""
import os
import sys
import yaml
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from models_v2.flow_matching_v2 import Sim2RealFlowModel
from utils_v2 import frequency_domain_loss, EarlyStopping
from critic_doppler import DopplerOnlyCritic, doppler_wgan_gp_loss, doppler_feature_matching_loss


class WGANTrainer:
    """
    V5 WGAN-GPè®­ç»ƒå™¨
    
    ä¸»è¦ç‰¹ç‚¹ï¼š
    1. ä½¿ç”¨WGAN-GPæ›¿ä»£ä¼ ç»ŸGAN
    2. æ›´ç¨³å®šçš„å¯¹æŠ—è®­ç»ƒ
    3. ç®€åŒ–çš„è¶…å‚æ•°è°ƒèŠ‚
    4. ä¿æŒå¤šæ™®å‹’ä¸“ç”¨ç‰¹æ€§
    """
    
    def __init__(self, config_path, pretrained_path):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config_path: str - é…ç½®æ–‡ä»¶è·¯å¾„
            pretrained_path: str - é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        """
        self.config_path = config_path
        self.pretrained_path = pretrained_path
        
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # è®¾å¤‡
        self.device = torch.device(self.config['device'])
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.setup_directories()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.setup_model()
        self.setup_critic()
        self.setup_optimizers()
        self.setup_training()
        
        # TensorBoard
        self.writer = SummaryWriter(self.config['paths']['log_dir'])
        
        print("âœ… V5 WGAN-GPè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def setup_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        for path_key in ['output_dir', 'log_dir', 'checkpoint_dir']:
            path = self.config['paths'][path_key]
            os.makedirs(path, exist_ok=True)
        print("âœ“ è¾“å‡ºç›®å½•åˆ›å»ºå®Œæˆ")
    
    def setup_model(self):
        """åŠ è½½é¢„è®­ç»ƒçš„Flow Matchingæ¨¡å‹"""
        model_cfg = self.config['model']
        
        self.model = Sim2RealFlowModel(
            base_channels=int(model_cfg['base_channels']),
            channel_mult=tuple(model_cfg['channel_mult']),
            time_embed_dim=int(model_cfg['time_embed_dim']),
            num_res_blocks=int(model_cfg['num_res_blocks']),
            attention_levels=tuple(model_cfg.get('attention_levels', [])),
            dropout=float(model_cfg.get('dropout', 0.0))
        ).to(self.device)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        print(f"åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {self.pretrained_path}")
        checkpoint = torch.load(self.pretrained_path, map_location=self.device, weights_only=False)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # è®¾ç½®å‚æ•°å†»ç»“ç­–ç•¥
        self.setup_parameter_groups()
        
        print("âœ“ Flow Matchingæ¨¡å‹åŠ è½½å®Œæˆ")
    
    def setup_parameter_groups(self):
        """è®¾ç½®å‚æ•°åˆ†ç»„ï¼ˆä¸V4ç›¸åŒçš„ç­–ç•¥ï¼‰"""
        freeze_mode = self.config['finetune'].get('freeze_mode', 'selective')
        
        if freeze_mode == 'all_trainable':
            for param in self.model.parameters():
                param.requires_grad = True
            print("å‚æ•°ç­–ç•¥: æ‰€æœ‰å‚æ•°å¯è®­ç»ƒ")
        
        elif freeze_mode == 'freeze_encoder':
            for name, param in self.model.named_parameters():
                if 'sim_encoder' in name:
                    param.requires_grad = False
                else:
                    param.requires_grad = True
            print("å‚æ•°ç­–ç•¥: å†»ç»“ç¼–ç å™¨ï¼Œå…¶ä»–å¯è®­ç»ƒ")
        
        elif freeze_mode == 'selective':
            for name, param in self.model.named_parameters():
                if 'sim_encoder' in name:
                    param.requires_grad = False
                elif 'time_embedding' in name or 'time_mlp' in name:
                    param.requires_grad = False
                elif 'down_blocks.0' in name or 'down_blocks.1' in name:
                    param.requires_grad = False
                elif 'up_blocks.3' in name or 'up_blocks.2' in name:
                    param.requires_grad = True
                else:
                    param.requires_grad = True
            print("å‚æ•°ç­–ç•¥: é€‰æ‹©æ€§å¾®è°ƒï¼ˆå†»ç»“ç¼–ç å™¨+ä½é¢‘å±‚ï¼‰")
        
        # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {trainable_params / total_params * 100:.2f}%")
    
    def setup_critic(self):
        """åˆ›å»ºWGAN-GP Critic"""
        critic_cfg = self.config['critic']
        
        self.critic = DopplerOnlyCritic(
            base_channels=int(critic_cfg.get('base_channels', 64)),
            dropout=float(critic_cfg.get('dropout', 0.3))
        ).to(self.device)
        
        print(f"âœ“ WGAN-GP Criticåˆ›å»ºå®Œæˆï¼ˆbase_channels={critic_cfg.get('base_channels', 64)}ï¼‰")
    
    def setup_optimizers(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        # ç”Ÿæˆå™¨ä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°ï¼‰
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        
        self.generator_optimizer = torch.optim.AdamW(
            trainable_params,
            lr=float(self.config['finetune']['lr_generator']),
            betas=tuple(self.config['train']['betas']),
            weight_decay=float(self.config['train']['weight_decay'])
        )
        
        # Criticä¼˜åŒ–å™¨ï¼ˆWGAN-GPæ¨èä½¿ç”¨Adamï¼Œbeta1=0, beta2=0.9ï¼‰
        self.critic_optimizer = torch.optim.Adam(
            self.critic.parameters(),
            lr=float(self.config['finetune']['lr_critic']),
            betas=(0.0, 0.9)  # WGAN-GPæ¨èè®¾ç½®
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        lr_scheduler_cfg = self.config['finetune'].get('lr_scheduler', {})
        if lr_scheduler_cfg.get('enabled', True):
            self.generator_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.generator_optimizer,
                mode='min',
                factor=float(lr_scheduler_cfg.get('factor', 0.7)),
                patience=int(lr_scheduler_cfg.get('patience', 10)),
                min_lr=float(lr_scheduler_cfg.get('min_lr', 1e-7))
            )
        else:
            self.generator_scheduler = None
        
        print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºå®Œæˆ")
        print(f"  ç”Ÿæˆå™¨å­¦ä¹ ç‡: {self.config['finetune']['lr_generator']}")
        print(f"  Criticå­¦ä¹ ç‡: {self.config['finetune']['lr_critic']}")
    
    def setup_training(self):
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        self.start_epoch = 1
        self.global_step = 0
        self.best_val_loss = float('inf')
        
        # WGAN-GPè®­ç»ƒå‚æ•°
        self.critic_update_freq = int(self.config['finetune'].get('critic_update_freq', 5))
        self.lambda_gp = float(self.config['finetune'].get('lambda_gp', 10.0))
        self.wgan_weight = float(self.config['finetune'].get('wgan_weight', 1.0))
        self.feature_matching_weight = float(self.config['finetune'].get('feature_matching_weight', 1.0))
        self.frequency_weight = float(self.config['loss'].get('frequency_weight', 1.5))
        
        # æ¢¯åº¦ç´¯ç§¯å‚æ•°
        self.gradient_accumulation_steps = int(self.config['train'].get('gradient_accumulation_steps', 1))
        
        print(f"\nâœ“ WGAN-GPè®­ç»ƒé…ç½®:")
        print(f"  Batch Size: {self.config['train']['batch_size']}")
        print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.gradient_accumulation_steps}")
        print(f"  ç­‰æ•ˆBatch Size: {self.config['train']['batch_size'] * self.gradient_accumulation_steps}")
        print(f"  Criticæ›´æ–°é¢‘ç‡: {self.critic_update_freq}")
        print(f"  æ¢¯åº¦æƒ©ç½šç³»æ•°: {self.lambda_gp}")
        print(f"  WGANæƒé‡: {self.wgan_weight}")
        
        # æ—©åœæœºåˆ¶
        early_stopping_cfg = self.config['finetune'].get('early_stopping', {})
        if early_stopping_cfg.get('enabled', True):
            self.early_stopping = EarlyStopping(
                patience=int(early_stopping_cfg.get('patience', 20)),
                min_delta=float(early_stopping_cfg.get('min_delta', 0.0001)),
                monitor=early_stopping_cfg.get('monitor', 'val_loss')
            )
        else:
            self.early_stopping = None
    
    def train_one_epoch(self, epoch, train_loader):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆWGAN-GPç‰ˆæœ¬ï¼‰"""
        self.model.train()
        self.critic.train()
        
        # åˆå§‹åŒ–æ¢¯åº¦
        self.generator_optimizer.zero_grad()
        self.critic_optimizer.zero_grad()
        
        total_loss_g = 0
        total_loss_c = 0
        total_loss_fm = 0
        total_loss_freq = 0
        total_loss_wgan = 0
        
        # Criticè¯„åˆ†ç»Ÿè®¡
        total_real_score = 0
        total_fake_score = 0
        total_score_gap = 0
        total_gp_loss = 0
        critic_updates = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
        
        for batch_idx, (sim_images, real_images, _) in enumerate(pbar):
            sim_images = sim_images.to(self.device)
            real_images = real_images.to(self.device)
            
            # ============================================================
            # é˜¶æ®µ1ï¼šè®­ç»ƒCriticï¼ˆæ¯Næ­¥æ›´æ–°ä¸€æ¬¡ï¼Œæ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼‰
            # ============================================================
            if batch_idx % self.critic_update_freq == 0:
                # ç”Ÿæˆå‡å›¾åƒ
                with torch.no_grad():
                    fake_images = self.model.generate(
                        sim_images,
                        ode_steps=int(self.config['finetune']['ode_steps']),
                        ode_method=self.config['finetune']['ode_method']
                    )
                
                # WGAN-GP CriticæŸå¤±
                c_loss, c_info = doppler_wgan_gp_loss(
                    self.critic, real_images, fake_images, 
                    mode='critic', lambda_gp=self.lambda_gp
                )
                
                # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±å½’ä¸€åŒ–
                c_loss = c_loss / self.gradient_accumulation_steps
                
                # åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ç´¯åŠ ï¼‰
                c_loss.backward()
                
                # è¾¾åˆ°ç´¯ç§¯æ­¥æ•°ï¼Œæ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°
                if (batch_idx // self.critic_update_freq + 1) % self.gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.critic.parameters(),
                        float(self.config['train']['max_grad_norm'])
                    )
                    self.critic_optimizer.step()
                    self.critic_optimizer.zero_grad()
                
                # ç»Ÿè®¡
                total_loss_c += c_loss.item() * self.gradient_accumulation_steps
                total_real_score += c_info['real_score']
                total_fake_score += c_info['fake_score']
                total_score_gap += c_info['score_gap']
                total_gp_loss += c_info['gp_loss']
                critic_updates += 1
                
                # TensorBoardè®°å½•
                if self.global_step % int(self.config['train']['log_interval']) == 0:
                    self.writer.add_scalar('train/loss_critic', c_loss.item() * self.gradient_accumulation_steps, self.global_step)
                    self.writer.add_scalar('train/real_score', c_info['real_score'], self.global_step)
                    self.writer.add_scalar('train/fake_score', c_info['fake_score'], self.global_step)
                    self.writer.add_scalar('train/score_gap', c_info['score_gap'], self.global_step)
                    self.writer.add_scalar('train/gradient_penalty', c_info['gp_loss'], self.global_step)
            
            # ============================================================
            # é˜¶æ®µ2ï¼šè®­ç»ƒç”Ÿæˆå™¨ï¼ˆæ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼‰
            # ============================================================
            # Flow Matching Loss
            loss_fm = self.model.compute_loss(sim_images, real_images)
            
            # è·å–é¢„æµ‹
            predicted = self.model.generate(
                sim_images,
                ode_steps=int(self.config['finetune']['ode_steps']),
                ode_method=self.config['finetune']['ode_method']
            )
            
            # é¢‘åŸŸLossï¼ˆä¿æŒåŸæœ‰èƒ½åŠ›ï¼‰
            loss_freq = torch.tensor(0.0, device=self.device)
            if self.config['loss'].get('use_frequency', False):
                loss_freq = frequency_domain_loss(predicted, real_images)
            
            # WGAN-GPå¯¹æŠ—æŸå¤±
            loss_wgan, wgan_info = doppler_wgan_gp_loss(
                self.critic, real_images, predicted, mode='generator'
            )
            
            # ç‰¹å¾åŒ¹é…æŸå¤±ï¼ˆè¾…åŠ©ï¼‰
            loss_fm_wgan, fm_info = doppler_feature_matching_loss(
                self.critic, real_images, predicted
            )
            
            # æ€»æŸå¤±
            loss_g = (
                loss_fm +
                self.frequency_weight * loss_freq +
                self.wgan_weight * (
                    loss_wgan +
                    self.feature_matching_weight * loss_fm_wgan
                )
            )
            
            # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±å½’ä¸€åŒ–
            loss_g = loss_g / self.gradient_accumulation_steps
            
            # åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ç´¯åŠ ï¼‰
            loss_g.backward()
            
            # æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(
                    [p for p in self.model.parameters() if p.requires_grad],
                    float(self.config['train']['max_grad_norm'])
                )
                self.generator_optimizer.step()
                self.generator_optimizer.zero_grad()
            
            # ç»Ÿè®¡
            total_loss_g += loss_g.item()
            total_loss_fm += loss_fm.item()
            if loss_freq.item() > 0:
                total_loss_freq += loss_freq.item()
            total_loss_wgan += loss_wgan.item()
            
            # TensorBoardæ—¥å¿—
            if self.global_step % int(self.config['train']['log_interval']) == 0:
                self.writer.add_scalar('train/loss_generator', loss_g.item() * self.gradient_accumulation_steps, self.global_step)
                self.writer.add_scalar('train/loss_fm', loss_fm.item(), self.global_step)
                if loss_freq.item() > 0:
                    self.writer.add_scalar('train/loss_frequency', loss_freq.item(), self.global_step)
                self.writer.add_scalar('train/loss_wgan', loss_wgan.item(), self.global_step)
                self.writer.add_scalar('train/loss_feature_matching', loss_fm_wgan.item(), self.global_step)
            
            # è¿›åº¦æ¡
            postfix = {
                'G': f"{loss_g.item():.4f}",
                'FM': f"{loss_fm.item():.4f}",
                'WGAN': f"{loss_wgan.item():.4f}",
            }
            if batch_idx % self.critic_update_freq == 0 and critic_updates > 0:
                postfix['C'] = f"{c_loss.item():.4f}"
                postfix['Real'] = f"{c_info['real_score']:.3f}"
                postfix['Fake'] = f"{c_info['fake_score']:.3f}"
                postfix['Gap'] = f"{c_info['score_gap']:.3f}"
            pbar.set_postfix(postfix)
            
            self.global_step += 1
        
        # Epochå¹³å‡
        n_batches = len(train_loader)
        c_updates = n_batches // self.critic_update_freq
        
        # æŸå¤±éœ€è¦ä¹˜ä»¥accumulation_stepsæ¢å¤åŸå§‹å°ºåº¦
        avg_loss_g = (total_loss_g * self.gradient_accumulation_steps) / n_batches
        avg_loss_c = (total_loss_c * self.gradient_accumulation_steps) / c_updates if c_updates > 0 else 0
        
        # WGAN-GPç»Ÿè®¡
        avg_real_score = total_real_score / critic_updates if critic_updates > 0 else 0
        avg_fake_score = total_fake_score / critic_updates if critic_updates > 0 else 0
        avg_score_gap = total_score_gap / critic_updates if critic_updates > 0 else 0
        avg_gp_loss = total_gp_loss / critic_updates if critic_updates > 0 else 0
        
        print(f"\nEpoch {epoch} æ€»ç»“:")
        print(f"  ç”Ÿæˆå™¨æŸå¤±: {avg_loss_g:.6f}")
        print(f"  CriticæŸå¤±: {avg_loss_c:.6f}")
        print(f"  çœŸå®å›¾åƒè¯„åˆ†: {avg_real_score:.4f}")
        print(f"  ç”Ÿæˆå›¾åƒè¯„åˆ†: {avg_fake_score:.4f}")
        print(f"  è¯„åˆ†å·®è·: {avg_score_gap:.4f}")
        print(f"  æ¢¯åº¦æƒ©ç½š: {avg_gp_loss:.6f}")
        print(f"  Criticæ›´æ–°æ¬¡æ•°: {critic_updates}")
        
        return avg_loss_g
    
    def validate(self, epoch, val_loader):
        """éªŒè¯"""
        self.model.eval()
        self.critic.eval()
        
        total_loss = 0
        total_loss_fm = 0
        total_loss_freq = 0
        total_real_score = 0
        total_fake_score = 0
        
        with torch.no_grad():
            for sim_images, real_images, _ in tqdm(val_loader, desc="éªŒè¯"):
                sim_images = sim_images.to(self.device)
                real_images = real_images.to(self.device)
                
                # Flow Matching Loss
                loss_fm = self.model.compute_loss(sim_images, real_images)
                
                # ç”Ÿæˆé¢„æµ‹
                predicted = self.model.generate(
                    sim_images,
                    ode_steps=int(self.config['finetune']['ode_steps']),
                    ode_method=self.config['finetune']['ode_method']
                )
                
                # é¢‘åŸŸLoss
                loss_freq = torch.tensor(0.0, device=self.device)
                if self.config['loss'].get('use_frequency', False):
                    loss_freq = frequency_domain_loss(predicted, real_images)
                
                # WGANè¯„åˆ†
                real_scores = self.critic(real_images)
                fake_scores = self.critic(predicted)
                
                # æ€»æŸå¤±ï¼ˆéªŒè¯æ—¶ä¸åŒ…å«å¯¹æŠ—æŸå¤±ï¼‰
                loss = loss_fm + self.frequency_weight * loss_freq
                
                total_loss += loss.item()
                total_loss_fm += loss_fm.item()
                if loss_freq.item() > 0:
                    total_loss_freq += loss_freq.item()
                total_real_score += real_scores.mean().item()
                total_fake_score += fake_scores.mean().item()
        
        avg_loss = total_loss / len(val_loader)
        avg_real_score = total_real_score / len(val_loader)
        avg_fake_score = total_fake_score / len(val_loader)
        
        self.writer.add_scalar('val/loss', avg_loss, epoch)
        self.writer.add_scalar('val/loss_fm', total_loss_fm / len(val_loader), epoch)
        if total_loss_freq > 0:
            self.writer.add_scalar('val/loss_freq', total_loss_freq / len(val_loader), epoch)
        self.writer.add_scalar('val/real_score', avg_real_score, epoch)
        self.writer.add_scalar('val/fake_score', avg_fake_score, epoch)
        
        # è®°å½•å­¦ä¹ ç‡
        current_lr = self.generator_optimizer.param_groups[0]['lr']
        self.writer.add_scalar('train/learning_rate', current_lr, epoch)
        
        print(f"éªŒè¯ç»“æœ: Loss={avg_loss:.6f}, Real_Score={avg_real_score:.4f}, Fake_Score={avg_fake_score:.4f}")
        
        return avg_loss
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'generator_optimizer_state_dict': self.generator_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'best_val_loss': self.best_val_loss,
            'global_step': self.global_step,
            'config': self.config
        }
        
        if self.generator_scheduler:
            checkpoint['scheduler_state_dict'] = self.generator_scheduler.state_dict()
        
        # ä¿å­˜å¸¸è§„æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(
            self.config['paths']['checkpoint_dir'],
            f'checkpoint_epoch_{epoch}.pth'
        )
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(
                self.config['paths']['checkpoint_dir'],
                'best_model.pth'
            )
            torch.save(checkpoint, best_path)
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
        
        print(f"âœ… ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    def train(self, train_loader, val_loader, num_epochs):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\nğŸš€ å¼€å§‹WGAN-GPè®­ç»ƒ...")
        print(f"è®­ç»ƒè½®æ•°: {num_epochs}")
        print(f"è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
        
        for epoch in range(self.start_epoch, num_epochs + 1):
            # è®­ç»ƒ
            train_loss = self.train_one_epoch(epoch, train_loader)
            
            # éªŒè¯
            val_loss = self.validate(epoch, val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.generator_scheduler:
                self.generator_scheduler.step(val_loss)
                if epoch > 1:
                    current_lr = self.generator_optimizer.param_groups[0]['lr']
                    print(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
            
            # æ£€æŸ¥æ˜¯å¦æœ€ä½³æ¨¡å‹
            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss
                print(f"ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {val_loss:.6f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            save_interval = int(self.config['train']['save_interval'])
            if epoch % save_interval == 0 or is_best:
                self.save_checkpoint(epoch, is_best)
            
            # æ—©åœæ£€æŸ¥
            if self.early_stopping:
                early_stop_triggered = self.early_stopping(val_loss)
                
                # è®°å½•æ—©åœçŠ¶æ€åˆ°TensorBoard
                self.writer.add_scalar('train/early_stopping_counter', self.early_stopping.counter, epoch)
                self.writer.add_scalar('train/early_stopping_patience', self.early_stopping.patience, epoch)
                if self.early_stopping.best_score is not None:
                    self.writer.add_scalar('train/early_stopping_best_score', self.early_stopping.best_score, epoch)
                
                if early_stop_triggered:
                    print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼åœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                    break
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = os.path.join(
            self.config['paths']['checkpoint_dir'],
            'final_model.pth'
        )
        final_checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        torch.save(final_checkpoint, final_path)
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
        print(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {final_path}")
        
        self.writer.close()
