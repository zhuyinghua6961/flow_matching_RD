"""
V4 - æ¸è¿›å¼å¾®è°ƒè®­ç»ƒå™¨
é˜¶æ®µ1ï¼šé¢„è®­ç»ƒFlow MatchingåŸºç¡€æ¨¡å‹ï¼ˆå¤–éƒ¨å®Œæˆï¼‰
é˜¶æ®µ2ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨GANä¸“é—¨ä¼˜åŒ–å¤šæ™®å‹’æ•ˆåº”
æ”¯æŒä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒï¼šç”Ÿæˆå™¨é¢„çƒ­ â†’ æ¸©å’Œå¯¹æŠ— â†’ æ­£å¸¸å¯¹æŠ—
"""
import os
from pathlib import Path
import yaml
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import numpy as np

from models_v2 import Sim2RealFlowModel
from utils_v2.losses import frequency_domain_loss, ssim_loss
from .discriminator_doppler import (
    DopplerOnlyDiscriminator,
    doppler_adversarial_loss,
    doppler_feature_matching_loss
)


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    def __init__(self, patience=20, min_delta=0.0001, monitor='val_loss'):
        self.patience = patience
        self.min_delta = min_delta
        self.monitor = monitor
        self.counter = 0
        self.best_score = None
        self.early_stop = False
    
    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
        elif score > self.best_score - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0
        
        return self.early_stop


class FineTuneTrainer:
    """
    æ¸è¿›å¼å¾®è°ƒè®­ç»ƒå™¨
    
    æ ¸å¿ƒç†å¿µï¼š
    1. åŠ è½½é¢„è®­ç»ƒçš„Flow Matchingæ¨¡å‹ï¼ˆåŸºç¡€èƒ½åŠ›å·²å…·å¤‡ï¼‰
    2. å†»ç»“å¤§éƒ¨åˆ†å‚æ•°ï¼ˆä¿æŠ¤èƒŒæ™¯å’Œæ•´ä½“ç»“æ„ï¼‰
    3. åªå¾®è°ƒä¸å¤šæ™®å‹’ç›¸å…³çš„é«˜é¢‘ç‰¹å¾
    4. ç”¨åˆ¤åˆ«å™¨ä¸“é—¨æŒ‡å¯¼å¤šæ™®å‹’æ•ˆåº”çš„æ”¹è¿›
    5. æ”¯æŒä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒï¼Œé¿å…åˆ¤åˆ«å™¨è¿‡æ—©å‹åˆ¶ç”Ÿæˆå™¨
    """
    
    def __init__(self, config_path, pretrained_checkpoint):
        """
        Args:
            config_path: str - é…ç½®æ–‡ä»¶è·¯å¾„
            pretrained_checkpoint: str - é¢„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        """
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.device = torch.device(self.config['train']['device'])
        
        # è®¾ç½®è·¯å¾„
        self.setup_paths()
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.load_pretrained_model(pretrained_checkpoint)
        
        # åˆ›å»ºåˆ¤åˆ«å™¨
        self.setup_discriminator()
        
        # è®¾ç½®å‚æ•°åˆ†ç»„å’Œä¼˜åŒ–å™¨
        self.setup_optimizers()
        
        # è®¾ç½®è®­ç»ƒ
        self.setup_training()
        
        # åˆå§‹åŒ–ä¸‰é˜¶æ®µè®­ç»ƒ
        self.setup_progressive_training()
        
        print("="*60)
        print("V4 å¾®è°ƒè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print("="*60)
        print(f"é¢„è®­ç»ƒæ¨¡å‹: {pretrained_checkpoint}")
        print(f"ç”Ÿæˆå™¨æ€»å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"ç”Ÿæˆå™¨å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}")
        print(f"åˆ¤åˆ«å™¨æ€»å‚æ•°: {sum(p.numel() for p in self.discriminator.parameters()):,}")
        print(f"åˆ¤åˆ«å™¨å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in self.discriminator.parameters() if p.requires_grad):,}")
        print("="*60)
    
    def setup_paths(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        self.output_dir = Path(self.config['paths']['output_dir'])
        self.log_dir = Path(self.config['paths']['log_dir'])
        self.checkpoint_dir = Path(self.config['paths']['checkpoint_dir'])
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        self.writer = SummaryWriter(log_dir=str(self.log_dir))
    
    def load_pretrained_model(self, checkpoint_path):
        """åŠ è½½é¢„è®­ç»ƒçš„Flow Matchingæ¨¡å‹"""
        print(f"\nåŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {checkpoint_path}")
        
        # åˆ›å»ºæ¨¡å‹
        model_cfg = self.config['model']
        self.model = Sim2RealFlowModel(
            base_channels=int(model_cfg['base_channels']),
            channel_mult=tuple(model_cfg['channel_mult']),
            time_embed_dim=int(model_cfg['time_embed_dim']),
            num_res_blocks=int(model_cfg['num_res_blocks']),
            attention_levels=tuple(model_cfg['attention_levels']),
            dropout=float(model_cfg['dropout'])
        ).to(self.device)
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼ˆEpoch {checkpoint['epoch']}ï¼‰")
        
        # å‚æ•°åˆ†ç»„ç­–ç•¥
        self.setup_parameter_groups()
    
    def setup_parameter_groups(self):
        """
        è®¾ç½®å‚æ•°åˆ†ç»„
        
        ç­–ç•¥ï¼š
        1. ç¼–ç å™¨ï¼ˆSimEncoderï¼‰ï¼šå†»ç»“æˆ–æä½å­¦ä¹ ç‡ï¼ˆèƒŒæ™¯ç‰¹å¾å·²å­¦å¥½ï¼‰
        2. UNetä½é¢‘éƒ¨åˆ†ï¼šä½å­¦ä¹ ç‡ï¼ˆæ•´ä½“ç»“æ„å·²ç¨³å®šï¼‰
        3. UNeté«˜é¢‘éƒ¨åˆ†ï¼šæ­£å¸¸å­¦ä¹ ç‡ï¼ˆå¤šæ™®å‹’ç›¸å…³ï¼Œéœ€è¦æ”¹è¿›ï¼‰
        4. Time Embeddingï¼šå†»ç»“ï¼ˆå·²å­¦å¥½ï¼‰
        """
        freeze_mode = self.config['finetune'].get('freeze_mode', 'selective')
        
        if freeze_mode == 'all_trainable':
            # æ¨¡å¼1ï¼šæ‰€æœ‰å‚æ•°éƒ½å¯è®­ç»ƒï¼ˆä¸æ¨èï¼‰
            for param in self.model.parameters():
                param.requires_grad = True
            print("å‚æ•°ç­–ç•¥: æ‰€æœ‰å‚æ•°å¯è®­ç»ƒ")
        
        elif freeze_mode == 'freeze_encoder':
            # æ¨¡å¼2ï¼šå†»ç»“ç¼–ç å™¨ï¼Œå…¶ä»–å¯è®­ç»ƒ
            for name, param in self.model.named_parameters():
                if 'sim_encoder' in name:
                    param.requires_grad = False
                else:
                    param.requires_grad = True
            print("å‚æ•°ç­–ç•¥: å†»ç»“ç¼–ç å™¨ï¼Œå…¶ä»–å¯è®­ç»ƒ")
        
        elif freeze_mode == 'selective':
            # æ¨¡å¼3ï¼šé€‰æ‹©æ€§å¾®è°ƒï¼ˆæ¨èï¼‰
            for name, param in self.model.named_parameters():
                if 'sim_encoder' in name:
                    # ç¼–ç å™¨ï¼šå†»ç»“
                    param.requires_grad = False
                elif 'time_embedding' in name or 'time_mlp' in name:
                    # æ—¶é—´åµŒå…¥ï¼šå†»ç»“
                    param.requires_grad = False
                elif 'down_blocks.0' in name or 'down_blocks.1' in name:
                    # UNetå‰ä¸¤å±‚ä¸‹é‡‡æ ·ï¼šå†»ç»“ï¼ˆä½é¢‘ç‰¹å¾ï¼‰
                    param.requires_grad = False
                elif 'up_blocks.3' in name or 'up_blocks.2' in name:
                    # UNetåä¸¤å±‚ä¸Šé‡‡æ ·ï¼šå¯è®­ç»ƒï¼ˆé«˜é¢‘ç‰¹å¾ï¼Œå¤šæ™®å‹’ç›¸å…³ï¼‰
                    param.requires_grad = True
                else:
                    # å…¶ä»–ï¼šå¯è®­ç»ƒ
                    param.requires_grad = True
            print("å‚æ•°ç­–ç•¥: é€‰æ‹©æ€§å¾®è°ƒï¼ˆå†»ç»“ç¼–ç å™¨+ä½é¢‘å±‚ï¼‰")
        
        else:
            raise ValueError(f"Unknown freeze_mode: {freeze_mode}")
        
        # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {trainable_params / total_params * 100:.2f}%")
    
    def setup_discriminator(self):
        """åˆ›å»ºå¤šæ™®å‹’åˆ¤åˆ«å™¨"""
        disc_cfg = self.config['discriminator']
        
        self.discriminator = DopplerOnlyDiscriminator(
            base_channels=int(disc_cfg.get('base_channels', 64)),
            dropout=float(disc_cfg.get('dropout', 0.3))
        ).to(self.device)
        
        print(f"åˆ¤åˆ«å™¨å·²åˆ›å»ºï¼ˆbase_channels={disc_cfg.get('base_channels', 64)}ï¼‰")
    
    def setup_optimizers(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        # ç”Ÿæˆå™¨ä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°ï¼‰
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        
        self.generator_optimizer = torch.optim.AdamW(
            trainable_params,
            lr=float(self.config['finetune']['lr_generator']),
            betas=tuple(self.config['train']['betas']),
            weight_decay=float(self.config['train']['weight_decay'])
        )
        
        # åˆ¤åˆ«å™¨ä¼˜åŒ–å™¨
        self.discriminator_optimizer = torch.optim.Adam(
            self.discriminator.parameters(),
            lr=float(self.config['finetune']['lr_discriminator']),
            betas=(0.5, 0.999)  # GANå¸¸ç”¨è®¾ç½®
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        lr_scheduler_cfg = self.config['finetune'].get('lr_scheduler', {})
        if lr_scheduler_cfg.get('enabled', True):
            self.generator_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.generator_optimizer,
                mode='min',
                factor=float(lr_scheduler_cfg.get('factor', 0.7)),
                patience=int(lr_scheduler_cfg.get('patience', 10)),
                min_lr=float(lr_scheduler_cfg.get('min_lr', 1e-7))
            )
        else:
            self.generator_scheduler = None
        
        print(f"ä¼˜åŒ–å™¨å·²åˆ›å»º")
        print(f"  ç”Ÿæˆå™¨å­¦ä¹ ç‡: {self.config['finetune']['lr_generator']}")
        print(f"  åˆ¤åˆ«å™¨å­¦ä¹ ç‡: {self.config['finetune']['lr_discriminator']}")
        if self.generator_scheduler:
            print(f"  å­¦ä¹ ç‡è°ƒåº¦å™¨: ReduceLROnPlateau (factor={lr_scheduler_cfg.get('factor', 0.7)}, patience={lr_scheduler_cfg.get('patience', 10)})")
    
    def setup_training(self):
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        self.start_epoch = 1  # ğŸ”§ ä¿®æ”¹ï¼šepochä»1å¼€å§‹
        self.global_step = 0
        self.best_val_loss = float('inf')
        
        # GANè®­ç»ƒå‚æ•°ï¼ˆåˆå§‹åŒ–ï¼Œå¯èƒ½ä¼šè¢«ä¸‰é˜¶æ®µè®­ç»ƒè¦†ç›–ï¼‰
        self.discriminator_update_freq = int(self.config['finetune'].get('discriminator_update_freq', 1))
        self.adversarial_weight = float(self.config['finetune'].get('adversarial_weight', 1.0))
        self.feature_matching_weight = float(self.config['finetune'].get('feature_matching_weight', 1.0))
        self.gan_weight = float(self.config['finetune'].get('gan_weight', 0.3))
        self.frequency_weight = float(self.config['loss'].get('frequency_weight', 1.5))
        
        # æ¢¯åº¦ç´¯ç§¯å‚æ•°
        self.gradient_accumulation_steps = int(self.config['train'].get('gradient_accumulation_steps', 1))
        
        print(f"\nè®­ç»ƒé…ç½®:")
        print(f"  Batch Size: {self.config['train']['batch_size']}")
        print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.gradient_accumulation_steps}")
        print(f"  ç­‰æ•ˆBatch Size: {self.config['train']['batch_size'] * self.gradient_accumulation_steps}")
        print(f"  åˆ¤åˆ«å™¨æ›´æ–°é¢‘ç‡: {self.discriminator_update_freq}")
        
        # æ—©åœæœºåˆ¶
        early_stopping_cfg = self.config['finetune'].get('early_stopping', {})
        if early_stopping_cfg.get('enabled', True):
            self.early_stopping = EarlyStopping(
                patience=int(early_stopping_cfg.get('patience', 20)),
                min_delta=float(early_stopping_cfg.get('min_delta', 0.0001)),
                monitor=early_stopping_cfg.get('monitor', 'val_loss')
            )
        else:
            self.early_stopping = None
    
    def setup_progressive_training(self):
        """è®¾ç½®ä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒ"""
        self.progressive_config = self.config['finetune'].get('progressive_training', {})
        self.progressive_enabled = self.progressive_config.get('enabled', False)
        self.current_stage = None
        self.current_stage_name = None
        
        # ä¿å­˜åŸå§‹é…ç½®ï¼ˆç”¨äºéæ¸è¿›å¼è®­ç»ƒï¼‰
        self.original_gan_weight = float(self.config['finetune'].get('gan_weight', 0.3))
        self.original_adversarial_weight = float(self.config['finetune'].get('adversarial_weight', 1.0))
        self.original_feature_matching_weight = float(self.config['finetune'].get('feature_matching_weight', 1.0))
        self.original_frequency_weight = float(self.config['loss'].get('frequency_weight', 1.5))
        self.original_lr_discriminator = float(self.config['finetune'].get('lr_discriminator', 1e-4))
        self.original_discriminator_update_freq = int(self.config['finetune'].get('discriminator_update_freq', 1))
        
        if self.progressive_enabled:
            print(f"\nğŸš€ ä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒå·²å¯ç”¨")
            print(f"  Stage 1: Epoch {self.progressive_config['stage1']['epochs'][0]}-{self.progressive_config['stage1']['epochs'][1]} - {self.progressive_config['stage1']['description']}")
            print(f"  Stage 2: Epoch {self.progressive_config['stage2']['epochs'][0]}-{self.progressive_config['stage2']['epochs'][1]} - {self.progressive_config['stage2']['description']}")
            print(f"  Stage 3: Epoch {self.progressive_config['stage3']['epochs'][0]}+ - {self.progressive_config['stage3']['description']}")
        else:
            print(f"\nğŸ“ ä½¿ç”¨å¸¸è§„è®­ç»ƒæ¨¡å¼ï¼ˆä¸‰é˜¶æ®µè®­ç»ƒå·²ç¦ç”¨ï¼‰")
    
    def get_current_stage_config(self, current_epoch):
        """æ ¹æ®å½“å‰epochç¡®å®šè®­ç»ƒé˜¶æ®µå¹¶è¿”å›å¯¹åº”é…ç½®"""
        if not self.progressive_enabled:
            return None, None
        
        # æ£€æŸ¥å½“å‰å¤„äºå“ªä¸ªé˜¶æ®µ
        for stage_name in ['stage1', 'stage2', 'stage3']:
            stage_config = self.progressive_config[stage_name]
            start_epoch, end_epoch = stage_config['epochs']
            if start_epoch <= current_epoch <= end_epoch:
                return stage_config, stage_name
        
        # å¦‚æœè¶…å‡ºæ‰€æœ‰é˜¶æ®µèŒƒå›´ï¼Œä½¿ç”¨stage3
        return self.progressive_config['stage3'], 'stage3'
    
    def apply_stage_config(self, stage_config, stage_name, current_epoch):
        """åº”ç”¨å½“å‰é˜¶æ®µçš„é…ç½®å‚æ•°"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢é˜¶æ®µ
        stage_changed = (self.current_stage_name != stage_name)
        
        if stage_changed:
            print(f"\nğŸ”„ åˆ‡æ¢åˆ° {stage_name} (Epoch {current_epoch}): {stage_config.get('description', '')}")
            
            # æ›´æ–°æŸå¤±æƒé‡
            self.gan_weight = float(stage_config.get('gan_weight', self.original_gan_weight))
            self.adversarial_weight = float(stage_config.get('adversarial_weight', self.original_adversarial_weight))
            self.feature_matching_weight = float(stage_config.get('feature_matching_weight', self.original_feature_matching_weight))
            self.frequency_weight = float(stage_config.get('frequency_weight', self.original_frequency_weight))
            
            # æ›´æ–°åˆ¤åˆ«å™¨å­¦ä¹ ç‡
            new_lr_discriminator = float(stage_config.get('lr_discriminator', self.original_lr_discriminator))
            for param_group in self.discriminator_optimizer.param_groups:
                param_group['lr'] = new_lr_discriminator
            
            # æ›´æ–°åˆ¤åˆ«å™¨æ›´æ–°é¢‘ç‡
            self.discriminator_update_freq = int(stage_config.get('discriminator_update_freq', self.original_discriminator_update_freq))
            
            # è®°å½•å‚æ•°å˜åŒ–
            print(f"   GANæƒé‡: {self.gan_weight}")
            print(f"   å¯¹æŠ—æŸå¤±æƒé‡: {self.adversarial_weight}")
            print(f"   ç‰¹å¾åŒ¹é…æƒé‡: {self.feature_matching_weight}")
            print(f"   é¢‘åŸŸæŸå¤±æƒé‡: {self.frequency_weight}")
            print(f"   åˆ¤åˆ«å™¨å­¦ä¹ ç‡: {new_lr_discriminator}")
            print(f"   åˆ¤åˆ«å™¨æ›´æ–°é¢‘ç‡: {self.discriminator_update_freq}")
            
            # æ›´æ–°å½“å‰é˜¶æ®µ
            self.current_stage = stage_config
            self.current_stage_name = stage_name
            
            # è®°å½•åˆ°TensorBoard
            self.writer.add_scalar('Training/Stage', {'stage1': 1, 'stage2': 2, 'stage3': 3}[stage_name], current_epoch)
            self.writer.add_scalar('Training/GAN_Weight', self.gan_weight, current_epoch)
            self.writer.add_scalar('Training/Discriminator_LR', new_lr_discriminator, current_epoch)
    
    def train_one_epoch(self, epoch, train_loader):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼‰"""
        # ğŸš€ ä¸‰é˜¶æ®µè®­ç»ƒï¼šæ£€æŸ¥å¹¶åº”ç”¨å½“å‰é˜¶æ®µé…ç½®
        stage_config, stage_name = self.get_current_stage_config(epoch)
        if stage_config is not None:
            self.apply_stage_config(stage_config, stage_name, epoch)
        
        self.model.train()
        self.discriminator.train()
        
        # åˆå§‹åŒ–æ¢¯åº¦ï¼ˆæ¢¯åº¦ç´¯ç§¯éœ€è¦ï¼‰
        self.generator_optimizer.zero_grad()
        self.discriminator_optimizer.zero_grad()
        
        total_loss_g = 0
        total_loss_d = 0
        total_loss_fm = 0
        total_loss_freq = 0
        total_loss_adv = 0
        
        # æ”¹è¿›çš„å‡†ç¡®ç‡ç»Ÿè®¡ï¼ˆç´¯ç§¯æ ·æœ¬æ•°è€Œä¸æ˜¯batchæ•°ï¼‰
        total_correct_real = 0
        total_correct_fake = 0
        total_samples_discriminator = 0
        
        # æ¢¯åº¦ç´¯ç§¯è®¡æ•°å™¨
        accum_steps = 0
        accum_correct_real = 0
        accum_correct_fake = 0
        accum_samples = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
        
        for batch_idx, (sim_images, real_images, _) in enumerate(pbar):
            sim_images = sim_images.to(self.device)
            real_images = real_images.to(self.device)
            
            # ============================================================
            # é˜¶æ®µ1ï¼šè®­ç»ƒåˆ¤åˆ«å™¨ï¼ˆæ¯Næ­¥æ›´æ–°ä¸€æ¬¡ï¼Œæ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼‰
            # ğŸš€ ä¸‰é˜¶æ®µè®­ç»ƒï¼šå¦‚æœåˆ¤åˆ«å™¨å­¦ä¹ ç‡ä¸º0ï¼Œè·³è¿‡åˆ¤åˆ«å™¨è®­ç»ƒ
            # ============================================================
            current_d_lr = self.discriminator_optimizer.param_groups[0]['lr']
            if batch_idx % self.discriminator_update_freq == 0 and current_d_lr > 0:
                # ç”Ÿæˆå‡å›¾åƒï¼ˆä½¿ç”¨é…ç½®çš„ODEæ¨ç†æ­¥æ•°ï¼Œæé«˜ç”Ÿæˆè´¨é‡ï¼‰
                with torch.no_grad():
                    fake_images = self.model.generate(
                        sim_images,
                        ode_steps=int(self.config['finetune']['ode_steps']),
                        ode_method=self.config['finetune']['ode_method']
                    )
                
                # åˆ¤åˆ«å™¨æŸå¤±
                d_loss, d_info = doppler_adversarial_loss(
                    self.discriminator, real_images, fake_images, mode='discriminator'
                )
                
                # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±å½’ä¸€åŒ–
                d_loss = d_loss / self.gradient_accumulation_steps
                
                # åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ç´¯åŠ ï¼‰
                d_loss.backward()
                
                # ç´¯ç§¯ç»Ÿè®¡ä¿¡æ¯
                accum_correct_real += d_info['num_correct_real']
                accum_correct_fake += d_info['num_correct_fake']
                accum_samples += d_info['num_samples']
                accum_steps += 1
                
                total_loss_d += d_loss.item() * self.gradient_accumulation_steps  # æ¢å¤åŸå§‹lossç”¨äºæ˜¾ç¤º
                
                # è¾¾åˆ°ç´¯ç§¯æ­¥æ•°ï¼Œæ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°
                if accum_steps >= self.gradient_accumulation_steps:
                    torch.nn.utils.clip_grad_norm_(
                        self.discriminator.parameters(),
                        float(self.config['train']['max_grad_norm'])
                    )
                    self.discriminator_optimizer.step()
                    self.discriminator_optimizer.zero_grad()
                    
                    # è®¡ç®—ç´¯ç§¯æœŸå†…çš„å‡†ç¡®ç‡ï¼ˆå¤šä¸ªæ ·æœ¬çš„å¹³å‡ï¼‰
                    accum_real_acc = accum_correct_real / accum_samples if accum_samples > 0 else 0
                    accum_fake_acc = accum_correct_fake / accum_samples if accum_samples > 0 else 0
                    
                    # è®°å½•ç´¯ç§¯æœŸå†…çš„æŸå¤±å’Œå‡†ç¡®ç‡åˆ°TensorBoardï¼ˆå¹³æ»‘å€¼ï¼‰
                    self.writer.add_scalar('train/loss_discriminator', d_loss.item() * self.gradient_accumulation_steps, self.global_step)
                    self.writer.add_scalar('train/d_real_acc', accum_real_acc, self.global_step)
                    self.writer.add_scalar('train/d_fake_acc', accum_fake_acc, self.global_step)
                    
                    # ç´¯åŠ åˆ°æ€»ç»Ÿè®¡
                    total_correct_real += accum_correct_real
                    total_correct_fake += accum_correct_fake
                    total_samples_discriminator += accum_samples
                    
                    # é‡ç½®ç´¯ç§¯å™¨
                    accum_steps = 0
                    accum_correct_real = 0
                    accum_correct_fake = 0
                    accum_samples = 0
            
            # ============================================================
            # é˜¶æ®µ2ï¼šè®­ç»ƒç”Ÿæˆå™¨ï¼ˆæ”¯æŒæ¢¯åº¦ç´¯ç§¯ï¼‰
            # ============================================================
            # Flow Matching Loss
            loss_fm = self.model.compute_loss(sim_images, real_images)
            
            # è·å–é¢„æµ‹ï¼ˆä½¿ç”¨é…ç½®çš„ODEæ¨ç†æ­¥æ•°ï¼Œä¸åˆ¤åˆ«å™¨è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
            predicted = self.model.generate(
                sim_images,
                ode_steps=int(self.config['finetune']['ode_steps']),
                ode_method=self.config['finetune']['ode_method']
            )
            
            # é¢‘åŸŸLossï¼ˆä¿æŒåŸæœ‰èƒ½åŠ›ï¼‰
            loss_freq = torch.tensor(0.0, device=self.device)
            if self.config['loss'].get('use_frequency', False):
                loss_freq = frequency_domain_loss(predicted, real_images)
            
            # GANå¯¹æŠ—æŸå¤±ï¼ˆä¸“é—¨ä¼˜åŒ–å¤šæ™®å‹’ï¼‰
            loss_adv, adv_info = doppler_adversarial_loss(
                self.discriminator, real_images, predicted, mode='generator'
            )
            
            # GANç‰¹å¾åŒ¹é…æŸå¤±ï¼ˆè¾…åŠ©ï¼‰
            loss_fm_gan, fm_info = doppler_feature_matching_loss(
                self.discriminator, real_images, predicted
            )
            
            # æ€»æŸå¤±ï¼ˆä½¿ç”¨åŠ¨æ€æƒé‡ï¼Œæ”¯æŒä¸‰é˜¶æ®µè®­ç»ƒï¼‰
            frequency_weight = getattr(self, 'frequency_weight', float(self.config['loss'].get('frequency_weight', 2.0)))
            gan_weight = getattr(self, 'gan_weight', float(self.config['finetune']['gan_weight']))
            
            loss_g = (
                loss_fm +
                frequency_weight * loss_freq +
                gan_weight * (
                    self.adversarial_weight * loss_adv +
                    self.feature_matching_weight * loss_fm_gan
                )
            )
            
            # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±å½’ä¸€åŒ–
            loss_g = loss_g / self.gradient_accumulation_steps
            
            # åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ç´¯åŠ ï¼‰
            loss_g.backward()
            
            # æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡
            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(
                    [p for p in self.model.parameters() if p.requires_grad],
                    float(self.config['train']['max_grad_norm'])
                )
                self.generator_optimizer.step()
                self.generator_optimizer.zero_grad()
            
            # ç»Ÿè®¡
            total_loss_g += loss_g.item()
            total_loss_fm += loss_fm.item()
            if loss_freq.item() > 0:
                total_loss_freq += loss_freq.item()
            total_loss_adv += loss_adv.item()
            
            # æ—¥å¿—
            if self.global_step % int(self.config['train']['log_interval']) == 0:
                self.writer.add_scalar('train/loss_generator', loss_g.item() * self.gradient_accumulation_steps, self.global_step)
                self.writer.add_scalar('train/loss_fm', loss_fm.item(), self.global_step)
                if loss_freq.item() > 0:
                    self.writer.add_scalar('train/loss_frequency', loss_freq.item(), self.global_step)
                self.writer.add_scalar('train/loss_adversarial', loss_adv.item(), self.global_step)
                self.writer.add_scalar('train/loss_feature_matching', loss_fm_gan.item(), self.global_step)
                # åˆ¤åˆ«å™¨æŸå¤±åœ¨ç´¯ç§¯æœŸç»“æŸæ—¶è®°å½•ï¼Œå‡†ç¡®ç‡ä¹Ÿåœ¨é‚£é‡Œè®°å½•
            
            # è¿›åº¦æ¡
            postfix = {
                'G': f"{loss_g.item():.4f}",
                'FM': f"{loss_fm.item():.4f}",
                'Adv': f"{loss_adv.item():.4f}",
            }
            # ğŸš€ ä¸‰é˜¶æ®µè®­ç»ƒï¼šæ˜¾ç¤ºåˆ¤åˆ«å™¨çŠ¶æ€
            current_d_lr = self.discriminator_optimizer.param_groups[0]['lr']
            if current_d_lr == 0:
                postfix['D'] = "SKIP"  # é˜¶æ®µ1ï¼šåˆ¤åˆ«å™¨è·³è¿‡
                postfix['Stage'] = getattr(self, 'current_stage_name', 'N/A')
            elif batch_idx % self.discriminator_update_freq == 0:
                postfix['D'] = f"{d_loss.item():.4f}"
                postfix['D_acc'] = f"{(d_info['real_acc'] + d_info['fake_acc'])/2:.2f}"
                if hasattr(self, 'current_stage_name'):
                    postfix['Stage'] = self.current_stage_name
            pbar.set_postfix(postfix)
            
            self.global_step += 1
        
        # Epochå¹³å‡
        n_batches = len(train_loader)
        d_updates = n_batches // self.discriminator_update_freq
        
        # æŸå¤±éœ€è¦ä¹˜ä»¥accumulation_stepsæ¢å¤åŸå§‹å°ºåº¦
        avg_loss_g = (total_loss_g * self.gradient_accumulation_steps) / n_batches
        avg_loss_d = (total_loss_d * self.gradient_accumulation_steps) / d_updates if d_updates > 0 else 0
        
        # ä½¿ç”¨æ”¹è¿›çš„å‡†ç¡®ç‡è®¡ç®—ï¼ˆåŸºäºæ ·æœ¬æ•°è€Œä¸æ˜¯batchæ•°ï¼‰
        avg_real_acc = total_correct_real / total_samples_discriminator if total_samples_discriminator > 0 else 0
        avg_fake_acc = total_correct_fake / total_samples_discriminator if total_samples_discriminator > 0 else 0
        
        print(f"\nEpoch {epoch} æ€»ç»“:")
        print(f"  ç”Ÿæˆå™¨æŸå¤±: {avg_loss_g:.6f}")
        print(f"  åˆ¤åˆ«å™¨æŸå¤±: {avg_loss_d:.6f}")
        print(f"  åˆ¤åˆ«å™¨å‡†ç¡®ç‡: Real={avg_real_acc:.4f}, Fake={avg_fake_acc:.4f}")
        print(f"  ç»Ÿè®¡æ ·æœ¬æ•°: {total_samples_discriminator}")
        
        return avg_loss_g
    
    @torch.no_grad()
    def validate(self, epoch, val_loader):
        """éªŒè¯"""
        self.model.eval()
        self.discriminator.eval()
        
        total_loss = 0
        total_loss_fm = 0
        total_loss_freq = 0
        
        for sim_images, real_images, _ in tqdm(val_loader, desc="Validation"):
            sim_images = sim_images.to(self.device)
            real_images = real_images.to(self.device)
            
            # Flow Matching Loss
            loss_fm = self.model.compute_loss(sim_images, real_images)
            
            # è·å–é¢„æµ‹ï¼ˆä½¿ç”¨é…ç½®çš„ODEæ¨ç†æ­¥æ•°ï¼Œä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
            predicted = self.model.generate(
                sim_images,
                ode_steps=int(self.config['finetune']['ode_steps']),
                ode_method=self.config['finetune']['ode_method']
            )
            
            # é¢‘åŸŸLoss
            loss_freq = torch.tensor(0.0, device=self.device)
            if self.config['loss'].get('use_frequency', False):
                loss_freq = frequency_domain_loss(predicted, real_images)
            
            # æ€»æŸå¤±
            loss = loss_fm + float(self.config['loss'].get('frequency_weight', 2.0)) * loss_freq
            
            total_loss += loss.item()
            total_loss_fm += loss_fm.item()
            if loss_freq.item() > 0:
                total_loss_freq += loss_freq.item()
        
        avg_loss = total_loss / len(val_loader)
        
        self.writer.add_scalar('val/loss', avg_loss, epoch)
        self.writer.add_scalar('val/loss_fm', total_loss_fm / len(val_loader), epoch)
        if total_loss_freq > 0:
            self.writer.add_scalar('val/loss_freq', total_loss_freq / len(val_loader), epoch)
        
        # è®°å½•å­¦ä¹ ç‡
        current_lr = self.generator_optimizer.param_groups[0]['lr']
        self.writer.add_scalar('train/learning_rate', current_lr, epoch)
        
        return avg_loss
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'discriminator_state_dict': self.discriminator.state_dict(),
            'generator_optimizer_state_dict': self.generator_optimizer.state_dict(),
            'discriminator_optimizer_state_dict': self.discriminator_optimizer.state_dict(),
            'generator_scheduler_state_dict': self.generator_scheduler.state_dict() if self.generator_scheduler else None,
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = self.checkpoint_dir / f"finetuned_epoch_{epoch}.pth"
        torch.save(checkpoint, checkpoint_path)
        print(f"ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = self.checkpoint_dir / "best_finetuned.pth"
            torch.save(checkpoint, best_path)
            print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
        
        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹ï¼ˆåªä¿ç•™æœ€è¿‘Nä¸ªï¼‰
        keep_n = int(self.config['train'].get('keep_last_n_checkpoints', 5))
        if keep_n > 0:
            checkpoints = sorted(
                self.checkpoint_dir.glob("finetuned_epoch_*.pth"),
                key=lambda p: int(p.stem.split('_')[-1])
            )
            if len(checkpoints) > keep_n:
                for old_ckpt in checkpoints[:-keep_n]:
                    old_ckpt.unlink()
                    print(f"  æ¸…ç†æ—§æ£€æŸ¥ç‚¹: {old_ckpt.name}")
    
    def save_final_model(self, epoch):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆè®­ç»ƒåœæ­¢æ—¶çš„å½“å‰æ¨¡å‹çŠ¶æ€ï¼‰"""
        final_path = self.checkpoint_dir / "final_finetuned.pth"
        
        checkpoint = {
            'epoch': epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'discriminator_state_dict': self.discriminator.state_dict(),
            'generator_optimizer_state_dict': self.generator_optimizer.state_dict(),
            'discriminator_optimizer_state_dict': self.discriminator_optimizer.state_dict(),
            'generator_scheduler_state_dict': self.generator_scheduler.state_dict() if self.generator_scheduler else None,
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        
        torch.save(checkpoint, final_path)
        print(f"âœ“ ä¿å­˜æœ€ç»ˆæ¨¡å‹ (Epoch {epoch}): {final_path}")
    
    def train(self, train_loader, val_loader, num_epochs):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\nå¼€å§‹å¾®è°ƒè®­ç»ƒ...")
        
        # ğŸ”§ ä¿®æ”¹ï¼šç¡®ä¿è®­ç»ƒå®Œæ•´çš„num_epochsä¸ªepochï¼ˆä»1åˆ°num_epochsï¼‰
        for epoch in range(self.start_epoch, num_epochs + 1):
            # è®­ç»ƒ
            train_loss = self.train_one_epoch(epoch, train_loader)
            
            # éªŒè¯
            val_loss = self.validate(epoch, val_loader)
            
            print(f"  Val Loss: {val_loss:.6f}")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.generator_scheduler:
                self.generator_scheduler.step(val_loss)
                current_lr = self.generator_optimizer.param_groups[0]['lr']
                if epoch > 1:  # ğŸ”§ ä¿®æ”¹ï¼šepochä»1å¼€å§‹ï¼Œæ‰€ä»¥ç¬¬2ä¸ªepochæ‰æ˜¾ç¤ºå­¦ä¹ ç‡å˜åŒ–
                    print(f"  å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss
                print(f"  âœ“ æ–°çš„æœ€ä½³æ¨¡å‹ï¼")
            
            # ğŸ”§ ä¿®æ”¹ï¼šepochä»1å¼€å§‹ï¼Œæ¯save_intervalä¸ªepochä¿å­˜ä¸€æ¬¡
            if epoch % int(self.config['train']['save_interval']) == 0 or is_best:
                self.save_checkpoint(epoch, is_best)
            
            # æ—©åœæ£€æŸ¥
            if self.early_stopping:
                # å…ˆè°ƒç”¨æ—©åœæ£€æŸ¥ï¼ˆä¼šæ›´æ–°counterï¼‰
                early_stop_triggered = self.early_stopping(val_loss)
                
                # è®°å½•æ›´æ–°åçš„æ—©åœè®¡æ•°å™¨åˆ°TensorBoard
                self.writer.add_scalar('train/early_stopping_counter', self.early_stopping.counter, epoch)
                self.writer.add_scalar('train/early_stopping_patience', self.early_stopping.patience, epoch)
                if self.early_stopping.best_score is not None:
                    self.writer.add_scalar('train/early_stopping_best_score', self.early_stopping.best_score, epoch)
                
                # æ‰“å°å½“å‰è®¡æ•°å™¨çŠ¶æ€
                if early_stop_triggered:
                    print(f"\næ—©åœè§¦å‘ï¼æœ€ä½³Val Loss: {self.best_val_loss:.6f}")
                    print(f"æ—©åœè®¡æ•°å™¨è¾¾åˆ°: {self.early_stopping.counter}/{self.early_stopping.patience}")
                    # æ—©åœæ—¶ä¿å­˜æœ€ç»ˆæ¨¡å‹
                    self.save_final_model(epoch)
                    break
                else:
                    # æ˜¾ç¤ºå½“å‰è®¡æ•°å™¨ï¼ˆ0è¡¨ç¤ºéªŒè¯æŸå¤±æœ‰æ”¹å–„ï¼Œ>0è¡¨ç¤ºè¿ç»­Nè½®æœªæ”¹å–„ï¼‰
                    print(f"  æ—©åœè®¡æ•°å™¨: {self.early_stopping.counter}/{self.early_stopping.patience} (0=æ”¹å–„ä¸­, {self.early_stopping.patience}=è§¦å‘)")
        else:
            # æ­£å¸¸è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
            print("\nè®­ç»ƒæ­£å¸¸å®Œæˆï¼")
            self.save_final_model(epoch)
        
        print(f"\nå¾®è°ƒè®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³Val Loss: {self.best_val_loss:.6f}")
        print(f"\nä¿å­˜çš„æ¨¡å‹:")
        print(f"  - best_finetuned.pth    (æœ€ä½³æ¨¡å‹)")
        print(f"  - final_finetuned.pth   (æœ€ç»ˆæ¨¡å‹ï¼Œç”¨äºæµ‹è¯•)")
        
        self.writer.close()
